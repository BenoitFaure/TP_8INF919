{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000, 1)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000, 1)\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Print dimensions of training data\n",
    "print(\"Training data shape: \", x_train.shape)\n",
    "print(\"Training labels shape: \", y_train.shape)\n",
    "\n",
    "# Print dimensions of test data\n",
    "print(\"Test data shape: \", x_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "\n",
    "# Print number of unique classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images by 10 degrees\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,  # randomly shift images vertically by 10% of the height\n",
    "    zoom_range=0.1,  # randomly zoom images by 10%\n",
    "    horizontal_flip=True  # randomly flip images horizontally\n",
    ")\n",
    "\n",
    "data_gen_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the ImageDataGenerator on x_train\n",
    "# datagen.fit(x_train)\n",
    "\n",
    "# # Generate augmented images\n",
    "# augmented_images = datagen.flow(x_train, y_train, batch_size=data_gen_batch_size)\n",
    "\n",
    "# # Iterate over the augmented images and append them to x_train\n",
    "# for i, (x_batch, y_batch) in enumerate(augmented_images):\n",
    "#     x_train = np.concatenate((x_train, x_batch), axis=0)\n",
    "#     y_train = np.concatenate((y_train, y_batch), axis=0)\n",
    "#     if i >= len(x_train) // data_gen_batch_size:\n",
    "#         break\n",
    "\n",
    "# # Print the new dimensions of x_train\n",
    "# print(\"New dimensions of x_train: \", x_train.shape)\n",
    "# print(\"New dimensions of y_train: \", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "resize = (32, 32) # (224, 224)\n",
    "\n",
    "# One hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Load as tf dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Normalize images\n",
    "def normalize_img(image, label):\n",
    "\n",
    "    # Resize images\n",
    "    image = tf.image.resize(image, resize)\n",
    "\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Normalise images\n",
    "ds_train = train_dataset.map(normalize_img)\n",
    "ds_test = test_dataset.map(normalize_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, \\\n",
    "    AveragePooling2D, BatchNormalization, Activation, Add, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, RandomZoom, RandomFlip, RandomTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initializer\n",
    "initializer = tf.initializers.he_normal(seed=42) # tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=42)\n",
    "classification_initializer = tf.initializers.glorot_normal(seed=42)\n",
    "\n",
    "activation_function = 'relu' # 'selu'\n",
    "use_bias = False # Batch Norm takes care of it\n",
    "epsilon_bn = 1.001e-5\n",
    "kernel_reg = None # tf.keras.regularizers.l2(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet identity block\n",
    "\n",
    "def identity_block(filter, kernel_size=3):\n",
    "    def _identity_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _identity_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet convolutional block\n",
    "\n",
    "def convolutional_res_block(filter, kernel_size=3):\n",
    "    def _convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3, epsilon=epsilon_bn)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reduction convolutional block\n",
    "\n",
    "def reduction_convolutional_res_block(filter, kernel_size=3, strides=2):\n",
    "    def _reduction_convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3, epsilon=epsilon_bn)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _reduction_convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional block\n",
    "def convolutional_block(filter, kernel_size=3, strides=2):\n",
    "    def _convolutional_block(x):\n",
    "        \n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = [\n",
    "    RandomRotation(0.1, seed=42),\n",
    "    RandomTranslation(0.1, 0.1, seed=42),\n",
    "    RandomZoom(0.1, seed=42),\n",
    "    RandomFlip(seed=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_5'\n",
    "\n",
    "# conv_net = [\n",
    "#     convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "#     MaxPooling2D(pool_size=3, strides = 2, padding = 'same'),\n",
    "\n",
    "#     identity_block(64),\n",
    "#     identity_block(64),\n",
    "\n",
    "#     convolutional_block(128),\n",
    "#     identity_block(128),\n",
    "\n",
    "#     convolutional_block(256),\n",
    "#     identity_block(256),\n",
    "\n",
    "#     convolutional_block(512),\n",
    "#     identity_block(512),\n",
    "\n",
    "#     AveragePooling2D(pool_size=7)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet9'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 3, strides = 1),\n",
    "\n",
    "    convolutional_block(128, kernel_size = 3, strides = 1),\n",
    "    MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "    \n",
    "    identity_block(128),\n",
    "\n",
    "    convolutional_block(256, kernel_size = 3, strides = 1),\n",
    "    MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "    convolutional_block(512, kernel_size = 3, strides = 1),\n",
    "    MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "    identity_block(512),\n",
    "    MaxPooling2D(pool_size=4, strides = 4, padding = 'same')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LittleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'littlenet'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(32, kernel_size = 3, strides = 2),\n",
    "    MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "    convolutional_block(64, kernel_size = 3, strides = 2),\n",
    "\n",
    "    convolutional_block(128, kernel_size = 3, strides = 2),\n",
    "    MaxPooling2D(pool_size=4, strides = 4, padding = 'same')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_name = None\n",
    "if False:\n",
    "    model_name = 'resnet_4_1'\n",
    "    load_model_name = 'resnet_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_net = [\n",
    "    Flatten(),\n",
    "    Dense(128, kernel_initializer=initializer, activation=activation_function),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, kernel_initializer=classification_initializer, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_net = [\n",
    "#     Flatten(),\n",
    "#     Dense(num_classes, kernel_initializer=classification_initializer, activation='softmax')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile mondel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "input_shape = (resize[0], resize[1], 3)\n",
    "input_layer = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "\n",
    "    def compile_layers(input, layers):\n",
    "        for layer in layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "    # Build Data augmentation\n",
    "    data_augmentation_layers = compile_layers(input_layer, data_aug)\n",
    "\n",
    "    # Build Feature Extractor\n",
    "    conv_net_layers = compile_layers(data_augmentation_layers, conv_net)\n",
    "\n",
    "    # Build Classifier\n",
    "    classification_layers = compile_layers(conv_net_layers, classification_net)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=classification_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_name is not None:\n",
    "    model.load_weights('models/' + load_model_name + '.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "random_rotation (RandomRotat (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "random_translation (RandomTr (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "random_zoom (RandomZoom)     (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "random_flip (RandomFlip)     (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 32)        864       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 64)          18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 128)         73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 111,722\n",
      "Trainable params: 111,274\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Train settings\n",
    "epochs = 200\n",
    "batch_size = 8 # 64\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 0.005\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate, clipvalue=0.1)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dataset for Performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# ds_train = ds_train.cache()\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "del x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model):\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(label_smoothing=0.2), metrics=['accuracy'])\n",
    "\n",
    "    # Save model callback\n",
    "    checkpoint = ModelCheckpoint('models/' + model_name + '.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "    # Tensorboard callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_' + model_name\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    print('tensorboard --logdir ' + log_dir)\n",
    "\n",
    "    # Train model\n",
    "    run_hist = model.fit(ds_train, validation_data=ds_test,\n",
    "                         epochs=epochs, batch_size=batch_size,\n",
    "                         callbacks=[checkpoint, tensorboard_callback])\n",
    "    \n",
    "    return run_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir logs/fit/20231124-210922_littlenet\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/88 [..............................] - ETA: 0s - loss: 4.1064 - accuracy: 0.1014WARNING:tensorflow:From c:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/88 [..............................] - ETA: 6s - loss: 3.7422 - accuracy: 0.1093WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0515s vs `on_train_batch_end` time: 0.0895s). Check your callbacks.\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.1762 - accuracy: 0.2496\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.22800, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 2.1762 - accuracy: 0.2496 - val_loss: 2.1589 - val_accuracy: 0.2280\n",
      "Epoch 2/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.9960 - accuracy: 0.3428\n",
      "Epoch 00002: val_accuracy improved from 0.22800 to 0.35530, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 66ms/step - loss: 1.9959 - accuracy: 0.3430 - val_loss: 2.0249 - val_accuracy: 0.3553\n",
      "Epoch 3/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.9400 - accuracy: 0.3820\n",
      "Epoch 00003: val_accuracy improved from 0.35530 to 0.42240, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.9399 - accuracy: 0.3822 - val_loss: 1.8771 - val_accuracy: 0.4224\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.9077 - accuracy: 0.4088\n",
      "Epoch 00004: val_accuracy improved from 0.42240 to 0.43700, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.9077 - accuracy: 0.4088 - val_loss: 1.8344 - val_accuracy: 0.4370\n",
      "Epoch 5/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8843 - accuracy: 0.4219\n",
      "Epoch 00005: val_accuracy improved from 0.43700 to 0.47630, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.8839 - accuracy: 0.4220 - val_loss: 1.7803 - val_accuracy: 0.4763\n",
      "Epoch 6/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8691 - accuracy: 0.4350\n",
      "Epoch 00006: val_accuracy did not improve from 0.47630\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.8689 - accuracy: 0.4351 - val_loss: 1.7871 - val_accuracy: 0.4719\n",
      "Epoch 7/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8566 - accuracy: 0.4419\n",
      "Epoch 00007: val_accuracy did not improve from 0.47630\n",
      "88/88 [==============================] - 6s 68ms/step - loss: 1.8565 - accuracy: 0.4419 - val_loss: 1.8243 - val_accuracy: 0.4482\n",
      "Epoch 8/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8384 - accuracy: 0.4533\n",
      "Epoch 00008: val_accuracy improved from 0.47630 to 0.47850, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.8381 - accuracy: 0.4533 - val_loss: 1.7773 - val_accuracy: 0.4785\n",
      "Epoch 9/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8231 - accuracy: 0.4632\n",
      "Epoch 00009: val_accuracy did not improve from 0.47850\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.8229 - accuracy: 0.4634 - val_loss: 1.7874 - val_accuracy: 0.4679\n",
      "Epoch 10/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8147 - accuracy: 0.4731\n",
      "Epoch 00010: val_accuracy did not improve from 0.47850\n",
      "88/88 [==============================] - 6s 67ms/step - loss: 1.8146 - accuracy: 0.4731 - val_loss: 1.8053 - val_accuracy: 0.4744\n",
      "Epoch 11/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8078 - accuracy: 0.4783\n",
      "Epoch 00011: val_accuracy improved from 0.47850 to 0.50140, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 67ms/step - loss: 1.8077 - accuracy: 0.4784 - val_loss: 1.7503 - val_accuracy: 0.5014\n",
      "Epoch 12/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7993 - accuracy: 0.4836\n",
      "Epoch 00012: val_accuracy did not improve from 0.50140\n",
      "88/88 [==============================] - 5s 60ms/step - loss: 1.7990 - accuracy: 0.4839 - val_loss: 1.8151 - val_accuracy: 0.4635\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.7881 - accuracy: 0.4903\n",
      "Epoch 00013: val_accuracy improved from 0.50140 to 0.51330, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.7881 - accuracy: 0.4903 - val_loss: 1.7392 - val_accuracy: 0.5133\n",
      "Epoch 14/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7810 - accuracy: 0.4935\n",
      "Epoch 00014: val_accuracy did not improve from 0.51330\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7808 - accuracy: 0.4937 - val_loss: 1.7727 - val_accuracy: 0.4935\n",
      "Epoch 15/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7739 - accuracy: 0.4998\n",
      "Epoch 00015: val_accuracy improved from 0.51330 to 0.52370, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.7738 - accuracy: 0.4999 - val_loss: 1.7246 - val_accuracy: 0.5237\n",
      "Epoch 16/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7691 - accuracy: 0.5045\n",
      "Epoch 00016: val_accuracy improved from 0.52370 to 0.53030, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7689 - accuracy: 0.5047 - val_loss: 1.7065 - val_accuracy: 0.5303\n",
      "Epoch 17/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7638 - accuracy: 0.5091\n",
      "Epoch 00017: val_accuracy improved from 0.53030 to 0.54620, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.7636 - accuracy: 0.5095 - val_loss: 1.6858 - val_accuracy: 0.5462\n",
      "Epoch 18/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7516 - accuracy: 0.5195\n",
      "Epoch 00018: val_accuracy did not improve from 0.54620\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.7515 - accuracy: 0.5196 - val_loss: 1.6870 - val_accuracy: 0.5424\n",
      "Epoch 19/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7442 - accuracy: 0.5218\n",
      "Epoch 00019: val_accuracy did not improve from 0.54620\n",
      "88/88 [==============================] - 5s 60ms/step - loss: 1.7439 - accuracy: 0.5221 - val_loss: 1.7234 - val_accuracy: 0.5167\n",
      "Epoch 20/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7380 - accuracy: 0.5261\n",
      "Epoch 00020: val_accuracy improved from 0.54620 to 0.55410, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7379 - accuracy: 0.5262 - val_loss: 1.6615 - val_accuracy: 0.5541\n",
      "Epoch 21/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7289 - accuracy: 0.5313\n",
      "Epoch 00021: val_accuracy did not improve from 0.55410\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7286 - accuracy: 0.5316 - val_loss: 1.6722 - val_accuracy: 0.5534\n",
      "Epoch 22/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7254 - accuracy: 0.5343\n",
      "Epoch 00022: val_accuracy did not improve from 0.55410\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7251 - accuracy: 0.5346 - val_loss: 1.6855 - val_accuracy: 0.5502\n",
      "Epoch 23/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7186 - accuracy: 0.5393\n",
      "Epoch 00023: val_accuracy did not improve from 0.55410\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7183 - accuracy: 0.5397 - val_loss: 1.7455 - val_accuracy: 0.5204\n",
      "Epoch 24/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7103 - accuracy: 0.5438\n",
      "Epoch 00024: val_accuracy did not improve from 0.55410\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.7101 - accuracy: 0.5438 - val_loss: 1.8277 - val_accuracy: 0.4839\n",
      "Epoch 25/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7068 - accuracy: 0.5458\n",
      "Epoch 00025: val_accuracy improved from 0.55410 to 0.55480, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.7066 - accuracy: 0.5460 - val_loss: 1.6736 - val_accuracy: 0.5548\n",
      "Epoch 26/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6993 - accuracy: 0.5522\n",
      "Epoch 00026: val_accuracy did not improve from 0.55480\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6990 - accuracy: 0.5523 - val_loss: 1.7031 - val_accuracy: 0.5381\n",
      "Epoch 27/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6971 - accuracy: 0.5538\n",
      "Epoch 00027: val_accuracy improved from 0.55480 to 0.56390, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6971 - accuracy: 0.5538 - val_loss: 1.6551 - val_accuracy: 0.5639\n",
      "Epoch 28/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6876 - accuracy: 0.5588\n",
      "Epoch 00028: val_accuracy did not improve from 0.56390\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6874 - accuracy: 0.5588 - val_loss: 1.6899 - val_accuracy: 0.5412\n",
      "Epoch 29/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6862 - accuracy: 0.5623\n",
      "Epoch 00029: val_accuracy improved from 0.56390 to 0.58320, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6862 - accuracy: 0.5623 - val_loss: 1.6173 - val_accuracy: 0.5832\n",
      "Epoch 30/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6831 - accuracy: 0.5623\n",
      "Epoch 00030: val_accuracy did not improve from 0.58320\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6827 - accuracy: 0.5626 - val_loss: 1.7038 - val_accuracy: 0.5419\n",
      "Epoch 31/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6740 - accuracy: 0.5695\n",
      "Epoch 00031: val_accuracy improved from 0.58320 to 0.59180, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6740 - accuracy: 0.5695 - val_loss: 1.6068 - val_accuracy: 0.5918\n",
      "Epoch 32/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6738 - accuracy: 0.5675\n",
      "Epoch 00032: val_accuracy did not improve from 0.59180\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6736 - accuracy: 0.5678 - val_loss: 1.6836 - val_accuracy: 0.5486\n",
      "Epoch 33/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6686 - accuracy: 0.5695\n",
      "Epoch 00033: val_accuracy did not improve from 0.59180\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6685 - accuracy: 0.5697 - val_loss: 1.6438 - val_accuracy: 0.5748\n",
      "Epoch 34/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6640 - accuracy: 0.5743\n",
      "Epoch 00034: val_accuracy did not improve from 0.59180\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6636 - accuracy: 0.5745 - val_loss: 1.6088 - val_accuracy: 0.5913\n",
      "Epoch 35/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6606 - accuracy: 0.5752\n",
      "Epoch 00035: val_accuracy improved from 0.59180 to 0.59700, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6604 - accuracy: 0.5753 - val_loss: 1.6024 - val_accuracy: 0.5970\n",
      "Epoch 36/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6591 - accuracy: 0.5802\n",
      "Epoch 00036: val_accuracy improved from 0.59700 to 0.60290, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6588 - accuracy: 0.5805 - val_loss: 1.5827 - val_accuracy: 0.6029\n",
      "Epoch 37/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6561 - accuracy: 0.5813\n",
      "Epoch 00037: val_accuracy did not improve from 0.60290\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6558 - accuracy: 0.5814 - val_loss: 1.6265 - val_accuracy: 0.5791\n",
      "Epoch 38/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6497 - accuracy: 0.5854\n",
      "Epoch 00038: val_accuracy improved from 0.60290 to 0.60620, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6491 - accuracy: 0.5857 - val_loss: 1.5871 - val_accuracy: 0.6062\n",
      "Epoch 39/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6501 - accuracy: 0.5852\n",
      "Epoch 00039: val_accuracy did not improve from 0.60620\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6498 - accuracy: 0.5854 - val_loss: 1.6225 - val_accuracy: 0.5852\n",
      "Epoch 40/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6448 - accuracy: 0.5868\n",
      "Epoch 00040: val_accuracy did not improve from 0.60620\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6445 - accuracy: 0.5871 - val_loss: 1.6081 - val_accuracy: 0.5966\n",
      "Epoch 41/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6426 - accuracy: 0.5910\n",
      "Epoch 00041: val_accuracy improved from 0.60620 to 0.60770, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6423 - accuracy: 0.5913 - val_loss: 1.5812 - val_accuracy: 0.6077\n",
      "Epoch 42/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6416 - accuracy: 0.5889\n",
      "Epoch 00042: val_accuracy did not improve from 0.60770\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6411 - accuracy: 0.5893 - val_loss: 1.5941 - val_accuracy: 0.6037\n",
      "Epoch 43/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6349 - accuracy: 0.5958\n",
      "Epoch 00043: val_accuracy improved from 0.60770 to 0.60980, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.6349 - accuracy: 0.5958 - val_loss: 1.5756 - val_accuracy: 0.6098\n",
      "Epoch 44/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6339 - accuracy: 0.5932\n",
      "Epoch 00044: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6338 - accuracy: 0.5933 - val_loss: 1.5828 - val_accuracy: 0.6090\n",
      "Epoch 45/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6329 - accuracy: 0.5943\n",
      "Epoch 00045: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6327 - accuracy: 0.5946 - val_loss: 1.6083 - val_accuracy: 0.5975\n",
      "Epoch 46/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6286 - accuracy: 0.5961\n",
      "Epoch 00046: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6284 - accuracy: 0.5962 - val_loss: 1.7191 - val_accuracy: 0.5382\n",
      "Epoch 47/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6290 - accuracy: 0.5990\n",
      "Epoch 00047: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6287 - accuracy: 0.5992 - val_loss: 1.5937 - val_accuracy: 0.6042\n",
      "Epoch 48/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6241 - accuracy: 0.6035\n",
      "Epoch 00048: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6236 - accuracy: 0.6038 - val_loss: 1.5818 - val_accuracy: 0.6072\n",
      "Epoch 49/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6241 - accuracy: 0.5995\n",
      "Epoch 00049: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6241 - accuracy: 0.5995 - val_loss: 1.6085 - val_accuracy: 0.5952\n",
      "Epoch 50/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6206 - accuracy: 0.6033\n",
      "Epoch 00050: val_accuracy did not improve from 0.60980\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6204 - accuracy: 0.6032 - val_loss: 1.6077 - val_accuracy: 0.5992\n",
      "Epoch 51/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6173 - accuracy: 0.6049\n",
      "Epoch 00051: val_accuracy improved from 0.60980 to 0.62560, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6170 - accuracy: 0.6051 - val_loss: 1.5618 - val_accuracy: 0.6256\n",
      "Epoch 52/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6155 - accuracy: 0.6073\n",
      "Epoch 00052: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6153 - accuracy: 0.6073 - val_loss: 1.5643 - val_accuracy: 0.6197\n",
      "Epoch 53/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6149 - accuracy: 0.6093\n",
      "Epoch 00053: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6145 - accuracy: 0.6095 - val_loss: 1.6178 - val_accuracy: 0.5915\n",
      "Epoch 54/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6143 - accuracy: 0.6077\n",
      "Epoch 00054: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.6143 - accuracy: 0.6077 - val_loss: 1.6323 - val_accuracy: 0.5832\n",
      "Epoch 55/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6124 - accuracy: 0.6067\n",
      "Epoch 00055: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.6122 - accuracy: 0.6070 - val_loss: 1.6092 - val_accuracy: 0.5911\n",
      "Epoch 56/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6111 - accuracy: 0.6097\n",
      "Epoch 00056: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6108 - accuracy: 0.6098 - val_loss: 1.6493 - val_accuracy: 0.5754\n",
      "Epoch 57/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6052 - accuracy: 0.6113\n",
      "Epoch 00057: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6050 - accuracy: 0.6114 - val_loss: 1.6233 - val_accuracy: 0.5869\n",
      "Epoch 58/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6019 - accuracy: 0.6134\n",
      "Epoch 00058: val_accuracy did not improve from 0.62560\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6015 - accuracy: 0.6136 - val_loss: 1.6132 - val_accuracy: 0.5951\n",
      "Epoch 59/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6062 - accuracy: 0.6119\n",
      "Epoch 00059: val_accuracy improved from 0.62560 to 0.63190, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6059 - accuracy: 0.6122 - val_loss: 1.5519 - val_accuracy: 0.6319\n",
      "Epoch 60/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6059 - accuracy: 0.6124\n",
      "Epoch 00060: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.6056 - accuracy: 0.6124 - val_loss: 1.6047 - val_accuracy: 0.5973\n",
      "Epoch 61/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6039 - accuracy: 0.6132\n",
      "Epoch 00061: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.6036 - accuracy: 0.6134 - val_loss: 1.7190 - val_accuracy: 0.5442\n",
      "Epoch 62/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6008 - accuracy: 0.6159\n",
      "Epoch 00062: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6007 - accuracy: 0.6161 - val_loss: 1.5993 - val_accuracy: 0.6030\n",
      "Epoch 63/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6002 - accuracy: 0.6144\n",
      "Epoch 00063: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5998 - accuracy: 0.6148 - val_loss: 1.5593 - val_accuracy: 0.6265\n",
      "Epoch 64/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5955 - accuracy: 0.6182\n",
      "Epoch 00064: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5955 - accuracy: 0.6182 - val_loss: 1.6262 - val_accuracy: 0.5837\n",
      "Epoch 65/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5964 - accuracy: 0.6177\n",
      "Epoch 00065: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5964 - accuracy: 0.6177 - val_loss: 1.5685 - val_accuracy: 0.6216\n",
      "Epoch 66/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5954 - accuracy: 0.6159\n",
      "Epoch 00066: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5951 - accuracy: 0.6160 - val_loss: 1.5876 - val_accuracy: 0.6068\n",
      "Epoch 67/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5934 - accuracy: 0.6204\n",
      "Epoch 00067: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5933 - accuracy: 0.6205 - val_loss: 1.6014 - val_accuracy: 0.5965\n",
      "Epoch 68/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5944 - accuracy: 0.6174\n",
      "Epoch 00068: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5939 - accuracy: 0.6179 - val_loss: 1.6517 - val_accuracy: 0.5744\n",
      "Epoch 69/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5929 - accuracy: 0.6197\n",
      "Epoch 00069: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5926 - accuracy: 0.6199 - val_loss: 1.5770 - val_accuracy: 0.6108\n",
      "Epoch 70/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5891 - accuracy: 0.6229\n",
      "Epoch 00070: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5890 - accuracy: 0.6229 - val_loss: 1.6254 - val_accuracy: 0.5887\n",
      "Epoch 71/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5867 - accuracy: 0.6243\n",
      "Epoch 00071: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5864 - accuracy: 0.6245 - val_loss: 1.6467 - val_accuracy: 0.5766\n",
      "Epoch 72/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5871 - accuracy: 0.6216\n",
      "Epoch 00072: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5869 - accuracy: 0.6216 - val_loss: 1.5945 - val_accuracy: 0.6079\n",
      "Epoch 73/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5874 - accuracy: 0.6237\n",
      "Epoch 00073: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5869 - accuracy: 0.6239 - val_loss: 1.5858 - val_accuracy: 0.6146\n",
      "Epoch 74/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5821 - accuracy: 0.6262\n",
      "Epoch 00074: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5816 - accuracy: 0.6265 - val_loss: 1.6199 - val_accuracy: 0.5933\n",
      "Epoch 75/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5854 - accuracy: 0.6226\n",
      "Epoch 00075: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5851 - accuracy: 0.6229 - val_loss: 1.5699 - val_accuracy: 0.6191\n",
      "Epoch 76/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5824 - accuracy: 0.6268\n",
      "Epoch 00076: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5822 - accuracy: 0.6270 - val_loss: 1.5642 - val_accuracy: 0.6255\n",
      "Epoch 77/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5809 - accuracy: 0.6269\n",
      "Epoch 00077: val_accuracy did not improve from 0.63190\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5804 - accuracy: 0.6272 - val_loss: 1.5696 - val_accuracy: 0.6138\n",
      "Epoch 78/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5815 - accuracy: 0.6265\n",
      "Epoch 00078: val_accuracy improved from 0.63190 to 0.63270, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5815 - accuracy: 0.6265 - val_loss: 1.5466 - val_accuracy: 0.6327\n",
      "Epoch 79/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5819 - accuracy: 0.6271\n",
      "Epoch 00079: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5815 - accuracy: 0.6275 - val_loss: 1.5698 - val_accuracy: 0.6212\n",
      "Epoch 80/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5807 - accuracy: 0.6285\n",
      "Epoch 00080: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5803 - accuracy: 0.6288 - val_loss: 1.5978 - val_accuracy: 0.5997\n",
      "Epoch 81/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5765 - accuracy: 0.6287\n",
      "Epoch 00081: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5763 - accuracy: 0.6288 - val_loss: 1.5788 - val_accuracy: 0.6168\n",
      "Epoch 82/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5766 - accuracy: 0.6276\n",
      "Epoch 00082: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5763 - accuracy: 0.6279 - val_loss: 1.6310 - val_accuracy: 0.5826\n",
      "Epoch 83/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5788 - accuracy: 0.6283\n",
      "Epoch 00083: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5786 - accuracy: 0.6284 - val_loss: 1.5993 - val_accuracy: 0.6039\n",
      "Epoch 84/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5738 - accuracy: 0.6283\n",
      "Epoch 00084: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5734 - accuracy: 0.6286 - val_loss: 1.5910 - val_accuracy: 0.6028\n",
      "Epoch 85/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5717 - accuracy: 0.6310\n",
      "Epoch 00085: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5714 - accuracy: 0.6313 - val_loss: 1.6327 - val_accuracy: 0.5808\n",
      "Epoch 86/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5710 - accuracy: 0.6304\n",
      "Epoch 00086: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5710 - accuracy: 0.6304 - val_loss: 1.5979 - val_accuracy: 0.6009\n",
      "Epoch 87/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5680 - accuracy: 0.6331\n",
      "Epoch 00087: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5680 - accuracy: 0.6331 - val_loss: 1.6096 - val_accuracy: 0.5938\n",
      "Epoch 88/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5695 - accuracy: 0.6338\n",
      "Epoch 00088: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5692 - accuracy: 0.6341 - val_loss: 1.6031 - val_accuracy: 0.6024\n",
      "Epoch 89/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5720 - accuracy: 0.6324\n",
      "Epoch 00089: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5717 - accuracy: 0.6327 - val_loss: 1.5648 - val_accuracy: 0.6171\n",
      "Epoch 90/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5688 - accuracy: 0.6341\n",
      "Epoch 00090: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5683 - accuracy: 0.6345 - val_loss: 1.6332 - val_accuracy: 0.5813\n",
      "Epoch 91/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5674 - accuracy: 0.6363\n",
      "Epoch 00091: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5670 - accuracy: 0.6363 - val_loss: 1.5588 - val_accuracy: 0.6258\n",
      "Epoch 92/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5660 - accuracy: 0.6342\n",
      "Epoch 00092: val_accuracy did not improve from 0.63270\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5660 - accuracy: 0.6342 - val_loss: 1.5634 - val_accuracy: 0.6221\n",
      "Epoch 93/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5680 - accuracy: 0.6353\n",
      "Epoch 00093: val_accuracy improved from 0.63270 to 0.63500, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5675 - accuracy: 0.6356 - val_loss: 1.5408 - val_accuracy: 0.6350\n",
      "Epoch 94/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5639 - accuracy: 0.6388\n",
      "Epoch 00094: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5635 - accuracy: 0.6389 - val_loss: 1.5701 - val_accuracy: 0.6206\n",
      "Epoch 95/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5632 - accuracy: 0.6375\n",
      "Epoch 00095: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5632 - accuracy: 0.6375 - val_loss: 1.6135 - val_accuracy: 0.5974\n",
      "Epoch 96/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5673 - accuracy: 0.6362\n",
      "Epoch 00096: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5669 - accuracy: 0.6364 - val_loss: 1.5735 - val_accuracy: 0.6135\n",
      "Epoch 97/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5639 - accuracy: 0.6340\n",
      "Epoch 00097: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5635 - accuracy: 0.6343 - val_loss: 1.6028 - val_accuracy: 0.5966\n",
      "Epoch 98/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5677 - accuracy: 0.6360\n",
      "Epoch 00098: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5677 - accuracy: 0.6361 - val_loss: 1.6495 - val_accuracy: 0.5840\n",
      "Epoch 99/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5604 - accuracy: 0.6395\n",
      "Epoch 00099: val_accuracy did not improve from 0.63500\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5601 - accuracy: 0.6396 - val_loss: 1.6145 - val_accuracy: 0.5897\n",
      "Epoch 100/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5609 - accuracy: 0.6382\n",
      "Epoch 00100: val_accuracy improved from 0.63500 to 0.64180, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5609 - accuracy: 0.6382 - val_loss: 1.5280 - val_accuracy: 0.6418\n",
      "Epoch 101/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5615 - accuracy: 0.6391\n",
      "Epoch 00101: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5613 - accuracy: 0.6393 - val_loss: 1.6247 - val_accuracy: 0.5902\n",
      "Epoch 102/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5565 - accuracy: 0.6442\n",
      "Epoch 00102: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5565 - accuracy: 0.6442 - val_loss: 1.6195 - val_accuracy: 0.5927\n",
      "Epoch 103/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5623 - accuracy: 0.6368 ETA: 1s -\n",
      "Epoch 00103: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5619 - accuracy: 0.6369 - val_loss: 1.5694 - val_accuracy: 0.6183\n",
      "Epoch 104/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5610 - accuracy: 0.6382\n",
      "Epoch 00104: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5607 - accuracy: 0.6382 - val_loss: 1.5642 - val_accuracy: 0.6225\n",
      "Epoch 105/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5573 - accuracy: 0.6431\n",
      "Epoch 00105: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5569 - accuracy: 0.6433 - val_loss: 1.5349 - val_accuracy: 0.6350\n",
      "Epoch 106/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5597 - accuracy: 0.6398\n",
      "Epoch 00106: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5593 - accuracy: 0.6399 - val_loss: 1.5937 - val_accuracy: 0.6043\n",
      "Epoch 107/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5572 - accuracy: 0.6433\n",
      "Epoch 00107: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5569 - accuracy: 0.6434 - val_loss: 1.6367 - val_accuracy: 0.5883\n",
      "Epoch 108/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5560 - accuracy: 0.6419\n",
      "Epoch 00108: val_accuracy did not improve from 0.64180\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5559 - accuracy: 0.6420 - val_loss: 1.6041 - val_accuracy: 0.5988\n",
      "Epoch 109/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5549 - accuracy: 0.6445\n",
      "Epoch 00109: val_accuracy improved from 0.64180 to 0.65530, saving model to models\\littlenet.h5\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5545 - accuracy: 0.6448 - val_loss: 1.5101 - val_accuracy: 0.6553\n",
      "Epoch 110/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5572 - accuracy: 0.6439\n",
      "Epoch 00110: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5568 - accuracy: 0.6442 - val_loss: 1.5925 - val_accuracy: 0.6081\n",
      "Epoch 111/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5537 - accuracy: 0.6431\n",
      "Epoch 00111: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5537 - accuracy: 0.6431 - val_loss: 1.5626 - val_accuracy: 0.6200\n",
      "Epoch 112/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5572 - accuracy: 0.6405\n",
      "Epoch 00112: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5568 - accuracy: 0.6408 - val_loss: 1.5691 - val_accuracy: 0.6194\n",
      "Epoch 113/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5551 - accuracy: 0.6438\n",
      "Epoch 00113: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5548 - accuracy: 0.6440 - val_loss: 1.5784 - val_accuracy: 0.6164\n",
      "Epoch 114/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5536 - accuracy: 0.6448\n",
      "Epoch 00114: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5530 - accuracy: 0.6450 - val_loss: 1.6100 - val_accuracy: 0.5967\n",
      "Epoch 115/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.6452\n",
      "Epoch 00115: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5528 - accuracy: 0.6452 - val_loss: 1.5254 - val_accuracy: 0.6379\n",
      "Epoch 116/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5506 - accuracy: 0.6456\n",
      "Epoch 00116: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5502 - accuracy: 0.6459 - val_loss: 1.5563 - val_accuracy: 0.6272\n",
      "Epoch 117/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5497 - accuracy: 0.6438\n",
      "Epoch 00117: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5494 - accuracy: 0.6440 - val_loss: 1.5824 - val_accuracy: 0.6079\n",
      "Epoch 118/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5521 - accuracy: 0.6462\n",
      "Epoch 00118: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5518 - accuracy: 0.6466 - val_loss: 1.6095 - val_accuracy: 0.6008\n",
      "Epoch 119/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5498 - accuracy: 0.6424\n",
      "Epoch 00119: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5494 - accuracy: 0.6426 - val_loss: 1.5808 - val_accuracy: 0.6108\n",
      "Epoch 120/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5461 - accuracy: 0.6510\n",
      "Epoch 00120: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5456 - accuracy: 0.6513 - val_loss: 1.5489 - val_accuracy: 0.6310\n",
      "Epoch 121/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5489 - accuracy: 0.6482\n",
      "Epoch 00121: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5488 - accuracy: 0.6482 - val_loss: 1.5665 - val_accuracy: 0.6202\n",
      "Epoch 122/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5480 - accuracy: 0.6467\n",
      "Epoch 00122: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5476 - accuracy: 0.6469 - val_loss: 1.5861 - val_accuracy: 0.6023\n",
      "Epoch 123/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5476 - accuracy: 0.6491\n",
      "Epoch 00123: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5472 - accuracy: 0.6494 - val_loss: 1.6236 - val_accuracy: 0.5901\n",
      "Epoch 124/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5452 - accuracy: 0.6506\n",
      "Epoch 00124: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5448 - accuracy: 0.6508 - val_loss: 1.5805 - val_accuracy: 0.6140\n",
      "Epoch 125/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5479 - accuracy: 0.6494\n",
      "Epoch 00125: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5475 - accuracy: 0.6497 - val_loss: 1.5464 - val_accuracy: 0.6340\n",
      "Epoch 126/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.6502\n",
      "Epoch 00126: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5455 - accuracy: 0.6505 - val_loss: 1.5629 - val_accuracy: 0.6251\n",
      "Epoch 127/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5448 - accuracy: 0.6505\n",
      "Epoch 00127: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5443 - accuracy: 0.6509 - val_loss: 1.6933 - val_accuracy: 0.5606\n",
      "Epoch 128/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5449 - accuracy: 0.6501\n",
      "Epoch 00128: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5447 - accuracy: 0.6503 - val_loss: 1.5674 - val_accuracy: 0.6217\n",
      "Epoch 129/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5437 - accuracy: 0.6513\n",
      "Epoch 00129: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5435 - accuracy: 0.6515 - val_loss: 1.5980 - val_accuracy: 0.6003\n",
      "Epoch 130/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5442 - accuracy: 0.6480\n",
      "Epoch 00130: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5440 - accuracy: 0.6483 - val_loss: 1.5702 - val_accuracy: 0.6178\n",
      "Epoch 131/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5484 - accuracy: 0.6485\n",
      "Epoch 00131: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5481 - accuracy: 0.6488 - val_loss: 1.5931 - val_accuracy: 0.6030\n",
      "Epoch 132/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5410 - accuracy: 0.6505\n",
      "Epoch 00132: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5410 - accuracy: 0.6505 - val_loss: 1.5693 - val_accuracy: 0.6194\n",
      "Epoch 133/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.6497\n",
      "Epoch 00133: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5455 - accuracy: 0.6501 - val_loss: 1.6017 - val_accuracy: 0.6013\n",
      "Epoch 134/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5414 - accuracy: 0.6520\n",
      "Epoch 00134: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5409 - accuracy: 0.6523 - val_loss: 1.5849 - val_accuracy: 0.6090\n",
      "Epoch 135/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5449 - accuracy: 0.6518\n",
      "Epoch 00135: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5449 - accuracy: 0.6518 - val_loss: 1.5429 - val_accuracy: 0.6277\n",
      "Epoch 136/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5412 - accuracy: 0.6489\n",
      "Epoch 00136: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5407 - accuracy: 0.6493 - val_loss: 1.5509 - val_accuracy: 0.6302\n",
      "Epoch 137/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5414 - accuracy: 0.6494\n",
      "Epoch 00137: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5409 - accuracy: 0.6497 - val_loss: 1.5525 - val_accuracy: 0.6271\n",
      "Epoch 138/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5414 - accuracy: 0.6511\n",
      "Epoch 00138: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5410 - accuracy: 0.6515 - val_loss: 1.5498 - val_accuracy: 0.6264\n",
      "Epoch 139/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5403 - accuracy: 0.6540\n",
      "Epoch 00139: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5401 - accuracy: 0.6543 - val_loss: 1.5778 - val_accuracy: 0.6211\n",
      "Epoch 140/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5405 - accuracy: 0.6538\n",
      "Epoch 00140: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5401 - accuracy: 0.6540 - val_loss: 1.5573 - val_accuracy: 0.6252\n",
      "Epoch 141/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5418 - accuracy: 0.6530\n",
      "Epoch 00141: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5414 - accuracy: 0.6534 - val_loss: 1.5631 - val_accuracy: 0.6212\n",
      "Epoch 142/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5405 - accuracy: 0.6518\n",
      "Epoch 00142: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.5403 - accuracy: 0.6519 - val_loss: 1.5391 - val_accuracy: 0.6347\n",
      "Epoch 143/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5353 - accuracy: 0.6537\n",
      "Epoch 00143: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5348 - accuracy: 0.6541 - val_loss: 1.5495 - val_accuracy: 0.6262\n",
      "Epoch 144/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5388 - accuracy: 0.6533\n",
      "Epoch 00144: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5385 - accuracy: 0.6535 - val_loss: 1.5596 - val_accuracy: 0.6238\n",
      "Epoch 145/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5410 - accuracy: 0.6502\n",
      "Epoch 00145: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5408 - accuracy: 0.6504 - val_loss: 1.5817 - val_accuracy: 0.6131\n",
      "Epoch 146/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5369 - accuracy: 0.6539\n",
      "Epoch 00146: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5365 - accuracy: 0.6539 - val_loss: 1.5622 - val_accuracy: 0.6198\n",
      "Epoch 147/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5353 - accuracy: 0.6552\n",
      "Epoch 00147: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5351 - accuracy: 0.6554 - val_loss: 1.5873 - val_accuracy: 0.6092\n",
      "Epoch 148/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5395 - accuracy: 0.6536\n",
      "Epoch 00148: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5391 - accuracy: 0.6538 - val_loss: 1.5769 - val_accuracy: 0.6082\n",
      "Epoch 149/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.6561\n",
      "Epoch 00149: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5360 - accuracy: 0.6563 - val_loss: 1.6091 - val_accuracy: 0.5958\n",
      "Epoch 150/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5344 - accuracy: 0.6561\n",
      "Epoch 00150: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5344 - accuracy: 0.6561 - val_loss: 1.6153 - val_accuracy: 0.5939\n",
      "Epoch 151/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.6571\n",
      "Epoch 00151: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5323 - accuracy: 0.6572 - val_loss: 1.5581 - val_accuracy: 0.6274\n",
      "Epoch 152/200\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5352 - accuracy: 0.6555\n",
      "Epoch 00152: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5352 - accuracy: 0.6555 - val_loss: 1.5525 - val_accuracy: 0.6281\n",
      "Epoch 153/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5390 - accuracy: 0.6534\n",
      "Epoch 00153: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5388 - accuracy: 0.6535 - val_loss: 1.5805 - val_accuracy: 0.6105\n",
      "Epoch 154/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5335 - accuracy: 0.6570\n",
      "Epoch 00154: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5332 - accuracy: 0.6571 - val_loss: 1.5187 - val_accuracy: 0.6472\n",
      "Epoch 155/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5351 - accuracy: 0.6578\n",
      "Epoch 00155: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 72ms/step - loss: 1.5347 - accuracy: 0.6580 - val_loss: 1.5813 - val_accuracy: 0.6103\n",
      "Epoch 156/200\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.5347 - accuracy: 0.6568\n",
      "Epoch 00156: val_accuracy did not improve from 0.65530\n",
      "88/88 [==============================] - 6s 74ms/step - loss: 1.5342 - accuracy: 0.6571 - val_loss: 1.6365 - val_accuracy: 0.5848\n",
      "Epoch 157/200\n",
      "66/88 [=====================>........] - ETA: 1s - loss: 1.5358 - accuracy: 0.6550"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benoi\\Documents\\zz_UQAC\\Apprentissage Automatique pour Données Maasives\\TP_8INF919\\devoir_2.ipynb Cell 53\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run train and validation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m run_hist \u001b[39m=\u001b[39m train_test_model(model)\n",
      "\u001b[1;32mc:\\Users\\benoi\\Documents\\zz_UQAC\\Apprentissage Automatique pour Données Maasives\\TP_8INF919\\devoir_2.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtensorboard --logdir \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m log_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m run_hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(ds_train, validation_data\u001b[39m=\u001b[39;49mds_test,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                      epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                      callbacks\u001b[39m=\u001b[39;49m[checkpoint, tensorboard_callback])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#X66sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m run_hist\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:103\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_method_wrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    105\u001b[0m   \u001b[39m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39mif\u001b[39;00m dc_context\u001b[39m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1093\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1087\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTraceContext\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1088\u001b[0m     graph_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1089\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1090\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1091\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size):\n\u001b[0;32m   1092\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1093\u001b[0m   tmp_logs \u001b[39m=\u001b[39m train_function(iterator)\n\u001b[0;32m   1094\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1095\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_filtered_call(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_filtered_call\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs, cancellation_manager\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[39m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[39m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   1844\u001b[0m       [t \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m nest\u001b[39m.\u001b[39;49mflatten((args, kwargs), expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1845\u001b[0m        \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(t, (ops\u001b[39m.\u001b[39;49mTensor,\n\u001b[0;32m   1846\u001b[0m                          resource_variable_ops\u001b[39m.\u001b[39;49mBaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m       captured_inputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m   1848\u001b[0m       cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1924\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    546\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    547\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    548\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    549\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    550\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    551\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run train and validation\n",
    "run_hist = train_test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "test_loss, test_acc = model.evaluate(ds_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.plot(run_hist.history['accuracy'], label='train')\n",
    "plt.plot(run_hist.history['val_accuracy'], label='test')\n",
    "plt.plot(run_hist.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-8inf919-aDMhdl4N-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
