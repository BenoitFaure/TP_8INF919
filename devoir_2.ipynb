{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000, 1)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000, 1)\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Print dimensions of training data\n",
    "print(\"Training data shape: \", x_train.shape)\n",
    "print(\"Training labels shape: \", y_train.shape)\n",
    "\n",
    "# Print dimensions of test data\n",
    "print(\"Test data shape: \", x_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "\n",
    "# Print number of unique classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images by 10 degrees\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,  # randomly shift images vertically by 10% of the height\n",
    "    zoom_range=0.1,  # randomly zoom images by 10%\n",
    "    horizontal_flip=True  # randomly flip images horizontally\n",
    ")\n",
    "\n",
    "data_gen_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the ImageDataGenerator on x_train\n",
    "# datagen.fit(x_train)\n",
    "\n",
    "# # Generate augmented images\n",
    "# augmented_images = datagen.flow(x_train, y_train, batch_size=data_gen_batch_size)\n",
    "\n",
    "# # Iterate over the augmented images and append them to x_train\n",
    "# for i, (x_batch, y_batch) in enumerate(augmented_images):\n",
    "#     x_train = np.concatenate((x_train, x_batch), axis=0)\n",
    "#     y_train = np.concatenate((y_train, y_batch), axis=0)\n",
    "#     if i >= len(x_train) // data_gen_batch_size:\n",
    "#         break\n",
    "\n",
    "# # Print the new dimensions of x_train\n",
    "# print(\"New dimensions of x_train: \", x_train.shape)\n",
    "# print(\"New dimensions of y_train: \", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "resize = (224, 224)\n",
    "\n",
    "# One hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Load as tf dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Normalize images\n",
    "def normalize_img(image, label):\n",
    "\n",
    "    # Resize images\n",
    "    image = tf.image.resize(image, resize)\n",
    "\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Normalise images\n",
    "ds_train = train_dataset.map(normalize_img)\n",
    "ds_test = test_dataset.map(normalize_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, \\\n",
    "    AveragePooling2D, BatchNormalization, Activation, Add, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, RandomZoom, RandomFlip, RandomTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initializer\n",
    "initializer = 'he_normal' # tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=42)\n",
    "classification_initializer = 'glorot_normal'\n",
    "\n",
    "activation_function = 'selu' # 'relu'\n",
    "kernel_reg = None # tf.keras.regularizers.l1_l2(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet identity block\n",
    "\n",
    "def identity_block(filter, kernel_size=3):\n",
    "    def _identity_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _identity_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet convolutional block\n",
    "\n",
    "def convolutional_res_block(filter, kernel_size=3):\n",
    "    def _convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reduction convolutional block\n",
    "\n",
    "def reduction_convolutional_res_block(filter, kernel_size=3, strides=2):\n",
    "    def _reduction_convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   strides=strides, kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _reduction_convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional block\n",
    "def convolutional_block(filter, kernel_size=3, strides=2):\n",
    "    def _convolutional_block(x):\n",
    "        \n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = [\n",
    "    RandomRotation(0.3, seed=42),\n",
    "    RandomTranslation(0.4, 0.4, seed=42),\n",
    "    RandomZoom(0.3, seed=42),\n",
    "    RandomFlip(seed=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = [\n",
    "    RandomRotation(0.1, seed=42),\n",
    "    RandomTranslation(0.1, 0.1, seed=42),\n",
    "    RandomZoom(0.1, seed=42),\n",
    "    RandomFlip(seed=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_1_data_augmentation'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64),\n",
    "    identity_block(64),\n",
    "    reduction_convolutional_res_block(128),\n",
    "    identity_block(128),\n",
    "    reduction_convolutional_res_block(256),\n",
    "    identity_block(256),\n",
    "    reduction_convolutional_res_block(512),\n",
    "    identity_block(512),\n",
    "    reduction_convolutional_res_block(512),\n",
    "    reduction_convolutional_res_block(512),\n",
    "    AveragePooling2D(pool_size=4, strides=4, padding='same'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_3'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64),\n",
    "    identity_block(64),\n",
    "    identity_block(64),\n",
    "    AveragePooling2D(pool_size=2, strides=2, padding='same'),\n",
    "    convolutional_res_block(128),\n",
    "    identity_block(128),\n",
    "    identity_block(128),\n",
    "    AveragePooling2D(pool_size=2, strides=2, padding='same'),\n",
    "    convolutional_res_block(256),\n",
    "    identity_block(256),\n",
    "    identity_block(256),\n",
    "    AveragePooling2D(pool_size=2, strides=2, padding='same'),\n",
    "    convolutional_res_block(512, kernel_size=1),\n",
    "    identity_block(512, kernel_size=1),\n",
    "    identity_block(512, kernel_size=1),\n",
    "    AveragePooling2D(pool_size=2, strides=2, padding='same')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_4'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "    AveragePooling2D(pool_size=3, strides = 2, padding = 'same'),\n",
    "    \n",
    "    identity_block(64),\n",
    "\n",
    "    AveragePooling2D(pool_size=(2, 2), padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(128),\n",
    "\n",
    "    AveragePooling2D(pool_size=(2, 2), padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(256),\n",
    "\n",
    "    AveragePooling2D(pool_size=(4, 4), padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(512),\n",
    "    \n",
    "    convolutional_block(512, kernel_size = 3, strides = 2),\n",
    "    AveragePooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_small_tests'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "\n",
    "    identity_block(64),\n",
    "\n",
    "    AveragePooling2D(pool_size=2, padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(128),\n",
    "\n",
    "    AveragePooling2D(pool_size=2, padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(256),\n",
    "\n",
    "    AveragePooling2D(pool_size=4, padding = 'same'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_4_32'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "\n",
    "    identity_block(64),\n",
    "\n",
    "    AveragePooling2D(pool_size=(2, 2), padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(128),\n",
    "\n",
    "    convolutional_res_block(256),\n",
    "\n",
    "    AveragePooling2D(pool_size=(4, 4), padding = 'same'),\n",
    "\n",
    "    convolutional_res_block(512),\n",
    "    \n",
    "    convolutional_block(512, kernel_size = 1, strides = 1),\n",
    "    AveragePooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_5'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "    MaxPooling2D(pool_size=3, strides = 2, padding = 'same'),\n",
    "\n",
    "    identity_block(64),\n",
    "    identity_block(64),\n",
    "\n",
    "    convolutional_block(128),\n",
    "    identity_block(128),\n",
    "\n",
    "    convolutional_block(256),\n",
    "    identity_block(256),\n",
    "\n",
    "    convolutional_block(512),\n",
    "    identity_block(512),\n",
    "\n",
    "    AveragePooling2D(pool_size=7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_name = None\n",
    "if False:\n",
    "    model_name = 'resnet_4_1'\n",
    "    load_model_name = 'resnet_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_net = [\n",
    "    Flatten(),\n",
    "    Dense(512, kernel_initializer=initializer, activation=activation_function),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, kernel_initializer=classification_initializer, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile mondel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "input_shape = (resize[0], resize[1], 3)\n",
    "input_layer = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "\n",
    "    def compile_layers(input, layers):\n",
    "        for layer in layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "    # Build Data augmentation\n",
    "    data_augmentation_layers = compile_layers(input_layer, data_aug)\n",
    "\n",
    "    # Build Feature Extractor\n",
    "    conv_net_layers = compile_layers(data_augmentation_layers, conv_net)\n",
    "\n",
    "    # Build Classifier\n",
    "    classification_layers = compile_layers(conv_net_layers, classification_net)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=classification_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_name is not None:\n",
    "    model.load_weights('models/' + load_model_name + '.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "random_rotation_1 (RandomRotati (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "random_translation_1 (RandomTra (None, 224, 224, 3)  0           random_rotation_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "random_zoom_1 (RandomZoom)      (None, 224, 224, 3)  0           random_translation_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "random_flip_1 (RandomFlip)      (None, 224, 224, 3)  0           random_zoom_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        random_flip_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 56, 56, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 64)   36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 56, 56, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 56, 56, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 64)   0           batch_normalization_2[0][0]      \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 64)   0           batch_normalization_4[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 128)  73856       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 128)  0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 28, 28, 128)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 256)  295168      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 14, 14, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 14, 14, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 256)  590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 14, 14, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 14, 14, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 14, 14, 256)  590080      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 14, 14, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 256)  0           batch_normalization_10[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 512)    1180160     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 7, 7, 512)    2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 7, 7, 512)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 512)    2359808     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 7, 7, 512)    2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 7, 7, 512)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 7, 7, 512)    2359808     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 7, 7, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 7, 7, 512)    0           batch_normalization_13[0][0]     \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 7, 7, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 1, 1, 512)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5130        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,181,130\n",
      "Trainable params: 8,175,114\n",
      "Non-trainable params: 6,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train settings\n",
    "epochs = 200\n",
    "batch_size = 64 # 8\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 0.005\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dataset for Performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# ds_train = ds_train.cache()\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "del x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model):\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(label_smoothing=0.2), metrics=['accuracy'])\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 50:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    # Save model callback\n",
    "    checkpoint = ModelCheckpoint('models/' + model_name + '.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "    # Tensorboard callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_' + model_name\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    print('tensorboard --logdir ' + log_dir)\n",
    "\n",
    "    # Train model\n",
    "    run_hist = model.fit(ds_train, validation_data=ds_test,\n",
    "                         epochs=epochs, batch_size=batch_size, \n",
    "                         \n",
    "                         callbacks=[checkpoint, tensorboard_callback])\n",
    "    \n",
    "    return run_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir logs/fit/20231123-142737_resnet_5\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/782 [..............................] - ETA: 0s - loss: 3.0173 - accuracy: 0.0625WARNING:tensorflow:From c:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/782 [..............................] - ETA: 2:38 - loss: 6.9851 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1524s vs `on_train_batch_end` time: 0.2545s). Check your callbacks.\n",
      "782/782 [==============================] - ETA: 0s - loss: 2.2212 - accuracy: 0.2786\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.15500, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 2.2212 - accuracy: 0.2786 - val_loss: 3.0384 - val_accuracy: 0.1550\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.9915 - accuracy: 0.3392\n",
      "Epoch 00002: val_accuracy improved from 0.15500 to 0.28200, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 196s 251ms/step - loss: 1.9915 - accuracy: 0.3392 - val_loss: 2.1527 - val_accuracy: 0.2820\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.9508 - accuracy: 0.3692\n",
      "Epoch 00003: val_accuracy improved from 0.28200 to 0.34270, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 254ms/step - loss: 1.9508 - accuracy: 0.3692 - val_loss: 2.2142 - val_accuracy: 0.3427\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.9069 - accuracy: 0.3977\n",
      "Epoch 00004: val_accuracy did not improve from 0.34270\n",
      "782/782 [==============================] - 196s 250ms/step - loss: 1.9069 - accuracy: 0.3977 - val_loss: 2.3487 - val_accuracy: 0.3269\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.8509 - accuracy: 0.4413\n",
      "Epoch 00005: val_accuracy improved from 0.34270 to 0.40640, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 196s 251ms/step - loss: 1.8509 - accuracy: 0.4413 - val_loss: 1.9954 - val_accuracy: 0.4064\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.8094 - accuracy: 0.4722\n",
      "Epoch 00006: val_accuracy did not improve from 0.40640\n",
      "782/782 [==============================] - 196s 250ms/step - loss: 1.8094 - accuracy: 0.4722 - val_loss: 2.1680 - val_accuracy: 0.3760\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.7678 - accuracy: 0.4992\n",
      "Epoch 00007: val_accuracy improved from 0.40640 to 0.45380, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 196s 250ms/step - loss: 1.7678 - accuracy: 0.4992 - val_loss: 1.9432 - val_accuracy: 0.4538\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.7302 - accuracy: 0.5242\n",
      "Epoch 00008: val_accuracy did not improve from 0.45380\n",
      "782/782 [==============================] - 195s 250ms/step - loss: 1.7302 - accuracy: 0.5242 - val_loss: 2.0157 - val_accuracy: 0.4052\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.6968 - accuracy: 0.5444\n",
      "Epoch 00009: val_accuracy did not improve from 0.45380\n",
      "782/782 [==============================] - 195s 250ms/step - loss: 1.6968 - accuracy: 0.5444 - val_loss: 2.1521 - val_accuracy: 0.3435\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.6525 - accuracy: 0.5721\n",
      "Epoch 00010: val_accuracy did not improve from 0.45380\n",
      "782/782 [==============================] - 195s 250ms/step - loss: 1.6525 - accuracy: 0.5721 - val_loss: 2.0126 - val_accuracy: 0.4343\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.6198 - accuracy: 0.5955\n",
      "Epoch 00011: val_accuracy improved from 0.45380 to 0.50390, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 196s 250ms/step - loss: 1.6198 - accuracy: 0.5955 - val_loss: 1.7697 - val_accuracy: 0.5039\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.5839 - accuracy: 0.6181\n",
      "Epoch 00012: val_accuracy improved from 0.50390 to 0.51320, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 196s 250ms/step - loss: 1.5839 - accuracy: 0.6181 - val_loss: 1.8134 - val_accuracy: 0.5132\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.5500 - accuracy: 0.6407\n",
      "Epoch 00013: val_accuracy improved from 0.51320 to 0.55410, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 200s 255ms/step - loss: 1.5500 - accuracy: 0.6407 - val_loss: 1.7244 - val_accuracy: 0.5541\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.5213 - accuracy: 0.6585\n",
      "Epoch 00014: val_accuracy did not improve from 0.55410\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.5213 - accuracy: 0.6585 - val_loss: 1.8916 - val_accuracy: 0.4911\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4982 - accuracy: 0.6720\n",
      "Epoch 00015: val_accuracy did not improve from 0.55410\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.4982 - accuracy: 0.6720 - val_loss: 1.8452 - val_accuracy: 0.5131\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4732 - accuracy: 0.6909\n",
      "Epoch 00016: val_accuracy did not improve from 0.55410\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.4732 - accuracy: 0.6909 - val_loss: 1.9714 - val_accuracy: 0.4835\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4529 - accuracy: 0.7042\n",
      "Epoch 00017: val_accuracy improved from 0.55410 to 0.59480, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 200s 256ms/step - loss: 1.4529 - accuracy: 0.7042 - val_loss: 1.6998 - val_accuracy: 0.5948\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4328 - accuracy: 0.7162\n",
      "Epoch 00018: val_accuracy did not improve from 0.59480\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.4328 - accuracy: 0.7162 - val_loss: 1.7735 - val_accuracy: 0.5547\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4139 - accuracy: 0.7297\n",
      "Epoch 00019: val_accuracy improved from 0.59480 to 0.62270, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.4139 - accuracy: 0.7297 - val_loss: 1.6074 - val_accuracy: 0.6227\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.4027 - accuracy: 0.7365\n",
      "Epoch 00020: val_accuracy did not improve from 0.62270\n",
      "782/782 [==============================] - 197s 253ms/step - loss: 1.4027 - accuracy: 0.7365 - val_loss: 1.6883 - val_accuracy: 0.5437\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3868 - accuracy: 0.7456\n",
      "Epoch 00021: val_accuracy did not improve from 0.62270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3868 - accuracy: 0.7456 - val_loss: 1.9075 - val_accuracy: 0.5018\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3708 - accuracy: 0.7546\n",
      "Epoch 00022: val_accuracy improved from 0.62270 to 0.64620, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.3708 - accuracy: 0.7546 - val_loss: 1.5512 - val_accuracy: 0.6462\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3614 - accuracy: 0.7621\n",
      "Epoch 00023: val_accuracy did not improve from 0.64620\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.3614 - accuracy: 0.7621 - val_loss: 1.6389 - val_accuracy: 0.6069\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3530 - accuracy: 0.7681\n",
      "Epoch 00024: val_accuracy improved from 0.64620 to 0.69380, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.3530 - accuracy: 0.7681 - val_loss: 1.5078 - val_accuracy: 0.6938\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3400 - accuracy: 0.7767\n",
      "Epoch 00025: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3400 - accuracy: 0.7767 - val_loss: 1.6501 - val_accuracy: 0.6507\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3304 - accuracy: 0.7826\n",
      "Epoch 00026: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.3304 - accuracy: 0.7826 - val_loss: 1.5299 - val_accuracy: 0.6853\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3241 - accuracy: 0.7840\n",
      "Epoch 00027: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3241 - accuracy: 0.7840 - val_loss: 1.6702 - val_accuracy: 0.6208\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.7912\n",
      "Epoch 00028: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3148 - accuracy: 0.7912 - val_loss: 1.9815 - val_accuracy: 0.5077\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3069 - accuracy: 0.7970\n",
      "Epoch 00029: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3069 - accuracy: 0.7970 - val_loss: 1.7026 - val_accuracy: 0.5863\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3009 - accuracy: 0.7998\n",
      "Epoch 00030: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.3009 - accuracy: 0.7998 - val_loss: 1.7501 - val_accuracy: 0.5697\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2910 - accuracy: 0.8054\n",
      "Epoch 00031: val_accuracy did not improve from 0.69380\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2910 - accuracy: 0.8054 - val_loss: 1.5741 - val_accuracy: 0.6801\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2890 - accuracy: 0.8075\n",
      "Epoch 00032: val_accuracy improved from 0.69380 to 0.69600, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.2890 - accuracy: 0.8075 - val_loss: 1.4941 - val_accuracy: 0.6960\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2791 - accuracy: 0.8128\n",
      "Epoch 00033: val_accuracy did not improve from 0.69600\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2791 - accuracy: 0.8128 - val_loss: 2.0792 - val_accuracy: 0.4275\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2733 - accuracy: 0.8175\n",
      "Epoch 00034: val_accuracy did not improve from 0.69600\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2733 - accuracy: 0.8175 - val_loss: 2.4863 - val_accuracy: 0.4164\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2682 - accuracy: 0.8198\n",
      "Epoch 00035: val_accuracy did not improve from 0.69600\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2682 - accuracy: 0.8198 - val_loss: 1.5538 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2615 - accuracy: 0.8238\n",
      "Epoch 00036: val_accuracy did not improve from 0.69600\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2615 - accuracy: 0.8238 - val_loss: 1.5714 - val_accuracy: 0.6757\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2565 - accuracy: 0.8288\n",
      "Epoch 00037: val_accuracy did not improve from 0.69600\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2565 - accuracy: 0.8288 - val_loss: 1.8114 - val_accuracy: 0.5667\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2494 - accuracy: 0.8321\n",
      "Epoch 00038: val_accuracy improved from 0.69600 to 0.76150, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.2494 - accuracy: 0.8321 - val_loss: 1.4506 - val_accuracy: 0.7615\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2459 - accuracy: 0.8346\n",
      "Epoch 00039: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.2459 - accuracy: 0.8346 - val_loss: 1.3921 - val_accuracy: 0.7473\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2452 - accuracy: 0.8341\n",
      "Epoch 00040: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2452 - accuracy: 0.8341 - val_loss: 1.5788 - val_accuracy: 0.7106\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2383 - accuracy: 0.8400\n",
      "Epoch 00041: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2383 - accuracy: 0.8400 - val_loss: 1.8457 - val_accuracy: 0.5792\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2366 - accuracy: 0.8387\n",
      "Epoch 00042: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2366 - accuracy: 0.8387 - val_loss: 1.6764 - val_accuracy: 0.6074\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2313 - accuracy: 0.8415\n",
      "Epoch 00043: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2313 - accuracy: 0.8415 - val_loss: 1.6605 - val_accuracy: 0.6433\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2254 - accuracy: 0.8469\n",
      "Epoch 00044: val_accuracy did not improve from 0.76150\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2254 - accuracy: 0.8469 - val_loss: 1.4465 - val_accuracy: 0.7542\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2198 - accuracy: 0.8496\n",
      "Epoch 00045: val_accuracy improved from 0.76150 to 0.78270, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.2198 - accuracy: 0.8496 - val_loss: 1.3515 - val_accuracy: 0.7827\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2170 - accuracy: 0.8517\n",
      "Epoch 00046: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2170 - accuracy: 0.8517 - val_loss: 1.4987 - val_accuracy: 0.7282\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2140 - accuracy: 0.8520\n",
      "Epoch 00047: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2140 - accuracy: 0.8520 - val_loss: 1.5889 - val_accuracy: 0.6934\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2099 - accuracy: 0.8560\n",
      "Epoch 00048: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2099 - accuracy: 0.8560 - val_loss: 2.1113 - val_accuracy: 0.4193\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2059 - accuracy: 0.8578\n",
      "Epoch 00049: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2059 - accuracy: 0.8578 - val_loss: 1.5947 - val_accuracy: 0.7030\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2023 - accuracy: 0.8614\n",
      "Epoch 00050: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.2023 - accuracy: 0.8614 - val_loss: 1.6348 - val_accuracy: 0.6542\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1983 - accuracy: 0.8629\n",
      "Epoch 00051: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1983 - accuracy: 0.8629 - val_loss: 1.4246 - val_accuracy: 0.7605\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1970 - accuracy: 0.8648\n",
      "Epoch 00052: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1970 - accuracy: 0.8648 - val_loss: 1.4476 - val_accuracy: 0.7326\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1929 - accuracy: 0.8679\n",
      "Epoch 00053: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1929 - accuracy: 0.8679 - val_loss: 1.3900 - val_accuracy: 0.7706\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1907 - accuracy: 0.8669\n",
      "Epoch 00054: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1907 - accuracy: 0.8669 - val_loss: 1.3785 - val_accuracy: 0.7644\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1885 - accuracy: 0.8697\n",
      "Epoch 00055: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1885 - accuracy: 0.8697 - val_loss: 1.4665 - val_accuracy: 0.7478\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1838 - accuracy: 0.8741\n",
      "Epoch 00056: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1838 - accuracy: 0.8741 - val_loss: 1.4909 - val_accuracy: 0.7311\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1834 - accuracy: 0.8721\n",
      "Epoch 00057: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1834 - accuracy: 0.8721 - val_loss: 1.3756 - val_accuracy: 0.7687\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1801 - accuracy: 0.8747\n",
      "Epoch 00058: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1801 - accuracy: 0.8747 - val_loss: 1.4040 - val_accuracy: 0.7622\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1762 - accuracy: 0.8780\n",
      "Epoch 00059: val_accuracy did not improve from 0.78270\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1762 - accuracy: 0.8780 - val_loss: 1.8518 - val_accuracy: 0.5923\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1712 - accuracy: 0.8809\n",
      "Epoch 00060: val_accuracy improved from 0.78270 to 0.79280, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1712 - accuracy: 0.8809 - val_loss: 1.3722 - val_accuracy: 0.7928\n",
      "Epoch 61/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1695 - accuracy: 0.8796\n",
      "Epoch 00061: val_accuracy improved from 0.79280 to 0.79910, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1695 - accuracy: 0.8796 - val_loss: 1.3415 - val_accuracy: 0.7991\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1667 - accuracy: 0.8829\n",
      "Epoch 00062: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1667 - accuracy: 0.8829 - val_loss: 1.5731 - val_accuracy: 0.6693\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1655 - accuracy: 0.8830\n",
      "Epoch 00063: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 253ms/step - loss: 1.1655 - accuracy: 0.8830 - val_loss: 2.0699 - val_accuracy: 0.5039\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1600 - accuracy: 0.8884\n",
      "Epoch 00064: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1600 - accuracy: 0.8884 - val_loss: 1.8550 - val_accuracy: 0.5581\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1603 - accuracy: 0.8859\n",
      "Epoch 00065: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1603 - accuracy: 0.8859 - val_loss: 1.5667 - val_accuracy: 0.6490\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1592 - accuracy: 0.8882\n",
      "Epoch 00066: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1592 - accuracy: 0.8882 - val_loss: 1.3650 - val_accuracy: 0.7914\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1563 - accuracy: 0.8892\n",
      "Epoch 00067: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1563 - accuracy: 0.8892 - val_loss: 1.5223 - val_accuracy: 0.7324\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1499 - accuracy: 0.8939\n",
      "Epoch 00068: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1499 - accuracy: 0.8939 - val_loss: 1.3986 - val_accuracy: 0.7368\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1498 - accuracy: 0.8934\n",
      "Epoch 00069: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1498 - accuracy: 0.8934 - val_loss: 1.3963 - val_accuracy: 0.7707\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1467 - accuracy: 0.8951\n",
      "Epoch 00070: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1467 - accuracy: 0.8951 - val_loss: 1.3388 - val_accuracy: 0.7857\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1440 - accuracy: 0.8982\n",
      "Epoch 00071: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1440 - accuracy: 0.8982 - val_loss: 1.3551 - val_accuracy: 0.7747\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1439 - accuracy: 0.8964\n",
      "Epoch 00072: val_accuracy did not improve from 0.79910\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1439 - accuracy: 0.8964 - val_loss: 1.5316 - val_accuracy: 0.7384\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1391 - accuracy: 0.8990\n",
      "Epoch 00073: val_accuracy improved from 0.79910 to 0.80520, saving model to models\\resnet_5.h5\n",
      "782/782 [==============================] - 198s 253ms/step - loss: 1.1391 - accuracy: 0.8990 - val_loss: 1.3083 - val_accuracy: 0.8052\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1366 - accuracy: 0.9002\n",
      "Epoch 00074: val_accuracy did not improve from 0.80520\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1366 - accuracy: 0.9002 - val_loss: 1.5743 - val_accuracy: 0.7172\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.8998\n",
      "Epoch 00075: val_accuracy did not improve from 0.80520\n",
      "782/782 [==============================] - 197s 252ms/step - loss: 1.1354 - accuracy: 0.8998 - val_loss: 1.4697 - val_accuracy: 0.7258\n",
      "Epoch 76/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1338 - accuracy: 0.9013\n",
      "Epoch 00076: val_accuracy did not improve from 0.80520\n",
      "782/782 [==============================] - 201s 257ms/step - loss: 1.1338 - accuracy: 0.9013 - val_loss: 1.4312 - val_accuracy: 0.7587\n",
      "Epoch 77/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1298 - accuracy: 0.9044\n",
      "Epoch 00077: val_accuracy did not improve from 0.80520\n",
      "782/782 [==============================] - 204s 260ms/step - loss: 1.1298 - accuracy: 0.9044 - val_loss: 1.4699 - val_accuracy: 0.7535\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.1289 - accuracy: 0.9054\n",
      "Epoch 00078: val_accuracy did not improve from 0.80520\n"
     ]
    }
   ],
   "source": [
    "# Run train and validation\n",
    "run_hist = train_test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "test_loss, test_acc = model.evaluate(ds_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.plot(run_hist.history['accuracy'], label='train')\n",
    "plt.plot(run_hist.history['val_accuracy'], label='test')\n",
    "plt.plot(run_hist.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-8inf919-aDMhdl4N-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
