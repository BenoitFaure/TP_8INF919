{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000, 1)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000, 1)\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Print dimensions of training data\n",
    "print(\"Training data shape: \", x_train.shape)\n",
    "print(\"Training labels shape: \", y_train.shape)\n",
    "\n",
    "# Print dimensions of test data\n",
    "print(\"Test data shape: \", x_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "\n",
    "# Print number of unique classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images by 10 degrees\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,  # randomly shift images vertically by 10% of the height\n",
    "    zoom_range=0.1,  # randomly zoom images by 10%\n",
    "    horizontal_flip=True  # randomly flip images horizontally\n",
    ")\n",
    "\n",
    "data_gen_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the ImageDataGenerator on x_train\n",
    "# datagen.fit(x_train)\n",
    "\n",
    "# # Generate augmented images\n",
    "# augmented_images = datagen.flow(x_train, y_train, batch_size=data_gen_batch_size)\n",
    "\n",
    "# # Iterate over the augmented images and append them to x_train\n",
    "# for i, (x_batch, y_batch) in enumerate(augmented_images):\n",
    "#     x_train = np.concatenate((x_train, x_batch), axis=0)\n",
    "#     y_train = np.concatenate((y_train, y_batch), axis=0)\n",
    "#     if i >= len(x_train) // data_gen_batch_size:\n",
    "#         break\n",
    "\n",
    "# # Print the new dimensions of x_train\n",
    "# print(\"New dimensions of x_train: \", x_train.shape)\n",
    "# print(\"New dimensions of y_train: \", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "resize = (224, 224) # (32, 32) # (224, 224)\n",
    "auto_encode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Load as tf dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Normalize images\n",
    "def normalize_img(image, label):\n",
    "\n",
    "    # Resize images\n",
    "    image = tf.image.resize(image, resize)\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    return image, label if not auto_encode else image\n",
    "\n",
    "# Normalise images\n",
    "ds_train = train_dataset.map(normalize_img)\n",
    "ds_test = test_dataset.map(normalize_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, \\\n",
    "    AveragePooling2D, BatchNormalization, Activation, Add, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, \\\n",
    "      RandomZoom, RandomFlip, RandomTranslation, Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initializer\n",
    "initializer = tf.initializers.he_normal(seed=42) # tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=42)\n",
    "classification_initializer = tf.initializers.glorot_normal(seed=42)\n",
    "\n",
    "activation_function = 'relu' # 'selu'\n",
    "use_bias = False # Batch Norm takes care of it\n",
    "epsilon_bn = 1.001e-5\n",
    "kernel_reg = None # tf.keras.regularizers.l2(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet identity block\n",
    "\n",
    "def identity_block(filter, kernel_size=3):\n",
    "    def _identity_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _identity_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resnet convolutional block\n",
    "\n",
    "def convolutional_res_block(filter, kernel_size=3):\n",
    "    def _convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3, epsilon=epsilon_bn)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "        \n",
    "        # Activation\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction Convolutional Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reduction convolutional block\n",
    "\n",
    "def reduction_convolutional_res_block(filter, kernel_size=3, strides=2):\n",
    "    def _reduction_convolutional_res_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(input_x)\n",
    "        input_x = BatchNormalization(axis=3, epsilon=epsilon_bn)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _reduction_convolutional_res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Deconv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeConv2D(filters, size, kernel_size, strides=1):\n",
    "    def _resize_conv2d(x):\n",
    "        x = Resizing(size, size, interpolation = 'nearest')(x)\n",
    "        x = Conv2D(filters, kernel_size, strides=strides, padding='same', kernel_initializer=initializer, use_bias=use_bias)(x)\n",
    "        return x\n",
    "    return _resize_conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResDecodeBlock(filters, size, kernel_size=3, strides=1):\n",
    "    def _res_decode_block(x):\n",
    "        input_x = x\n",
    "\n",
    "        # Layer 1\n",
    "        x = ResizeConv2D(filters, size, kernel_size)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = Conv2D(filters, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "\n",
    "        # Layer 3\n",
    "        input_x = ResizeConv2D(filters, size, kernel_size)(input_x)\n",
    "        input_x = BatchNormalization(axis=3, epsilon=epsilon_bn)(input_x)\n",
    "\n",
    "        # Add Residue\n",
    "        x = Add()([x, input_x])\n",
    "\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _res_decode_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional block\n",
    "def convolutional_block(filter, kernel_size=3, strides=2):\n",
    "    def _convolutional_block(x):\n",
    "        \n",
    "        x = Conv2D(filter, kernel_size=kernel_size, \n",
    "                   padding = 'same', kernel_initializer=initializer,\n",
    "                   use_bias=use_bias, strides=strides, kernel_regularizer=kernel_reg)(x)\n",
    "        x = BatchNormalization(axis=3, epsilon=epsilon_bn)(x)\n",
    "        x = Activation(activation_function)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    return _convolutional_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not auto_encode:\n",
    "    \n",
    "    data_aug = [\n",
    "        RandomRotation(0.1, seed=42),\n",
    "        RandomTranslation(0.1, 0.1, seed=42),\n",
    "        RandomZoom(0.1, seed=42),\n",
    "        RandomFlip(seed=42),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet_18' # 'resnet_5'\n",
    "\n",
    "conv_net = [\n",
    "    convolutional_block(64, kernel_size = 7, strides = 2),\n",
    "    AveragePooling2D(pool_size=3, strides = 2, padding = 'same'),\n",
    "\n",
    "    identity_block(64),\n",
    "    identity_block(64),\n",
    "\n",
    "    convolutional_block(128),\n",
    "    identity_block(128),\n",
    "\n",
    "    convolutional_block(256),\n",
    "    identity_block(256),\n",
    "\n",
    "    convolutional_block(512),\n",
    "    identity_block(512),\n",
    "\n",
    "    AveragePooling2D(pool_size=7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "\n",
    "if auto_encode:\n",
    "\n",
    "    decoder_net = [\n",
    "        \n",
    "        Resizing(7, 7),#, interpolation='nearest'),\n",
    "\n",
    "        ResDecodeBlock(512, 14),\n",
    "        ResDecodeBlock(256, 28),\n",
    "        ResDecodeBlock(128, 56),\n",
    "        ResDecodeBlock(64, 112),\n",
    "\n",
    "        ResizeConv2D(3, 224, 7),\n",
    "        Activation('sigmoid')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'resnet9'\n",
    "\n",
    "# conv_net = [\n",
    "#     convolutional_block(64, kernel_size = 3, strides = 1),\n",
    "\n",
    "#     convolutional_block(128, kernel_size = 3, strides = 1),\n",
    "#     MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "    \n",
    "#     identity_block(128),\n",
    "\n",
    "#     convolutional_block(256, kernel_size = 3, strides = 1),\n",
    "#     MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "#     convolutional_block(512, kernel_size = 3, strides = 1),\n",
    "#     MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "#     identity_block(512),\n",
    "#     MaxPooling2D(pool_size=4, strides = 4, padding = 'same')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LittleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'littlenet'\n",
    "\n",
    "# conv_net = [\n",
    "#     convolutional_block(32, kernel_size = 3, strides = 2),\n",
    "#     MaxPooling2D(pool_size=2, strides = 2, padding = 'same'),\n",
    "\n",
    "#     convolutional_block(64, kernel_size = 3, strides = 2),\n",
    "\n",
    "#     convolutional_block(128, kernel_size = 3, strides = 2),\n",
    "#     MaxPooling2D(pool_size=4, strides = 4, padding = 'same')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_name = None\n",
    "if False:\n",
    "    model_name = 'resnet_4_1'\n",
    "    load_model_name = 'resnet_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not auto_encode:\n",
    "\n",
    "    classification_net = [\n",
    "        Flatten(),\n",
    "        Dense(512, kernel_initializer=initializer, activation=activation_function),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, kernel_initializer=classification_initializer, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_net = [\n",
    "#     Flatten(),\n",
    "#     Dense(num_classes, kernel_initializer=classification_initializer, activation='softmax')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "input_shape = (resize[0], resize[1], 3)\n",
    "input_layer = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "\n",
    "    def compile_layers(input, layers):\n",
    "        for layer in layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "    output_layer = None\n",
    "    \n",
    "    if not auto_encode:\n",
    "\n",
    "        # Build Data augmentation\n",
    "        data_augmentation_layers = compile_layers(input_layer, data_aug)\n",
    "\n",
    "        # Build Feature Extractor\n",
    "        conv_net_layers = compile_layers(data_augmentation_layers, conv_net)\n",
    "\n",
    "        # Build Classifier\n",
    "        classification_layers = compile_layers(conv_net_layers, classification_net)\n",
    "\n",
    "        output_layer = classification_layers\n",
    "\n",
    "    else:\n",
    "            \n",
    "            # Build Feature Extractor\n",
    "            conv_net_layers = compile_layers(input_layer, conv_net)\n",
    "    \n",
    "            # Build Decoder\n",
    "            decoder_layers = compile_layers(conv_net_layers, decoder_net)\n",
    "\n",
    "            output_layer = decoder_layers\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_name is not None:\n",
    "    model.load_weights('models/' + load_model_name + '.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "random_rotation (RandomRotation (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "random_translation (RandomTrans (None, 224, 224, 3)  0           random_rotation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "random_zoom (RandomZoom)        (None, 224, 224, 3)  0           random_translation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "random_flip (RandomFlip)        (None, 224, 224, 3)  0           random_zoom[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9408        random_flip[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 56, 56, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 64)   36864       average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 56, 56, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 64)   36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 56, 56, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 64)   0           batch_normalization_2[0][0]      \n",
      "                                                                 average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 64)   36864       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 64)   36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 64)   0           batch_normalization_4[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 128)  73728       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 128)  147456      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 128)  147456      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 128)  0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 28, 28, 128)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 256)  294912      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 14, 14, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 14, 14, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 256)  589824      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 14, 14, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 14, 14, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 14, 14, 256)  589824      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 14, 14, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 256)  0           batch_normalization_10[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 512)    1179648     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 7, 7, 512)    2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 7, 7, 512)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 512)    2359296     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 7, 7, 512)    2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 7, 7, 512)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 7, 7, 512)    2359296     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 7, 7, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 7, 7, 512)    0           batch_normalization_13[0][0]     \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 7, 7, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5130        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,178,122\n",
      "Trainable params: 8,172,106\n",
      "Non-trainable params: 6,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Train settings\n",
    "epochs = 300\n",
    "batch_size = 8 # 64\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 0.005\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate) #, clipvalue=0.1)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dataset for Performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# ds_train = ds_train.cache()\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "del x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model):\n",
    "\n",
    "    if not auto_encode:\n",
    "        loss = CategoricalCrossentropy(label_smoothing=0.2)\n",
    "    else:\n",
    "        loss = 'mse'\n",
    "\n",
    "    name_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_' + model_name\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    # Save model callback\n",
    "    checkpoint = ModelCheckpoint('models/' + name_str + '.h5', monitor='val_accuracy', \n",
    "                                save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "    # Tensorboard callback\n",
    "    log_dir = \"logs/fit/\" + name_str\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    print('tensorboard --logdir ' + log_dir)\n",
    "\n",
    "    # Train model\n",
    "    run_hist = model.fit(ds_train, validation_data=ds_test,\n",
    "                         epochs=epochs, batch_size=batch_size,\n",
    "                         callbacks=[checkpoint, tensorboard_callback])\n",
    "    \n",
    "    return run_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir logs/fit/20231128-102227_resnet_18\n",
      "Epoch 1/300\n",
      "   1/6250 [..............................] - ETA: 12s - loss: 3.0639 - accuracy: 0.0000e+00WARNING:tensorflow:From c:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/6250 [..............................] - ETA: 12:23 - loss: 4.5209 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0381s vs `on_train_batch_end` time: 0.1979s). Check your callbacks.\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 2.2375 - accuracy: 0.1728\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25980, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 329s 53ms/step - loss: 2.2375 - accuracy: 0.1728 - val_loss: 2.0703 - val_accuracy: 0.2598\n",
      "Epoch 2/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 2.0851 - accuracy: 0.2663\n",
      "Epoch 00002: val_accuracy improved from 0.25980 to 0.34060, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 54ms/step - loss: 2.0851 - accuracy: 0.2663 - val_loss: 1.9295 - val_accuracy: 0.3406\n",
      "Epoch 3/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 2.0009 - accuracy: 0.3250\n",
      "Epoch 00003: val_accuracy improved from 0.34060 to 0.40830, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 2.0009 - accuracy: 0.3250 - val_loss: 1.8566 - val_accuracy: 0.4083\n",
      "Epoch 4/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.9210 - accuracy: 0.3851\n",
      "Epoch 00004: val_accuracy improved from 0.40830 to 0.47110, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.9210 - accuracy: 0.3851 - val_loss: 1.7568 - val_accuracy: 0.4711\n",
      "Epoch 5/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.8530 - accuracy: 0.4396\n",
      "Epoch 00005: val_accuracy improved from 0.47110 to 0.52830, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.8530 - accuracy: 0.4396 - val_loss: 1.6872 - val_accuracy: 0.5283\n",
      "Epoch 6/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.7961 - accuracy: 0.4841\n",
      "Epoch 00006: val_accuracy improved from 0.52830 to 0.57860, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.7961 - accuracy: 0.4841 - val_loss: 1.6137 - val_accuracy: 0.5786\n",
      "Epoch 7/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.7481 - accuracy: 0.5222\n",
      "Epoch 00007: val_accuracy improved from 0.57860 to 0.60800, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.7481 - accuracy: 0.5222 - val_loss: 1.5760 - val_accuracy: 0.6080\n",
      "Epoch 8/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.7124 - accuracy: 0.5463\n",
      "Epoch 00008: val_accuracy improved from 0.60800 to 0.63150, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.7124 - accuracy: 0.5463 - val_loss: 1.5388 - val_accuracy: 0.6315\n",
      "Epoch 9/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.6803 - accuracy: 0.5693\n",
      "Epoch 00009: val_accuracy improved from 0.63150 to 0.66120, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.6803 - accuracy: 0.5693 - val_loss: 1.4996 - val_accuracy: 0.6612\n",
      "Epoch 10/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.6529 - accuracy: 0.5902\n",
      "Epoch 00010: val_accuracy did not improve from 0.66120\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.6529 - accuracy: 0.5902 - val_loss: 1.5191 - val_accuracy: 0.6511\n",
      "Epoch 11/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.6239 - accuracy: 0.6126\n",
      "Epoch 00011: val_accuracy improved from 0.66120 to 0.67140, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.6239 - accuracy: 0.6126 - val_loss: 1.5074 - val_accuracy: 0.6714\n",
      "Epoch 12/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.5967 - accuracy: 0.6308\n",
      "Epoch 00012: val_accuracy improved from 0.67140 to 0.69620, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 331s 53ms/step - loss: 1.5967 - accuracy: 0.6308 - val_loss: 1.4546 - val_accuracy: 0.6962\n",
      "Epoch 13/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.5718 - accuracy: 0.6475\n",
      "Epoch 00013: val_accuracy did not improve from 0.69620\n",
      "6250/6250 [==============================] - 330s 53ms/step - loss: 1.5718 - accuracy: 0.6475 - val_loss: 1.4738 - val_accuracy: 0.6885\n",
      "Epoch 14/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.5480 - accuracy: 0.6648\n",
      "Epoch 00014: val_accuracy improved from 0.69620 to 0.71390, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.5480 - accuracy: 0.6648 - val_loss: 1.4289 - val_accuracy: 0.7139\n",
      "Epoch 15/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.5276 - accuracy: 0.6795\n",
      "Epoch 00015: val_accuracy improved from 0.71390 to 0.73330, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.5276 - accuracy: 0.6795 - val_loss: 1.3955 - val_accuracy: 0.7333\n",
      "Epoch 16/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.5080 - accuracy: 0.6902\n",
      "Epoch 00016: val_accuracy improved from 0.73330 to 0.74660, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.5080 - accuracy: 0.6901 - val_loss: 1.3598 - val_accuracy: 0.7466\n",
      "Epoch 17/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.4949 - accuracy: 0.6984\n",
      "Epoch 00017: val_accuracy did not improve from 0.74660\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.4949 - accuracy: 0.6984 - val_loss: 1.3748 - val_accuracy: 0.7450\n",
      "Epoch 18/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4762 - accuracy: 0.7097\n",
      "Epoch 00018: val_accuracy did not improve from 0.74660\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.4762 - accuracy: 0.7097 - val_loss: 1.3696 - val_accuracy: 0.7440\n",
      "Epoch 19/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.4637 - accuracy: 0.7201\n",
      "Epoch 00019: val_accuracy improved from 0.74660 to 0.75360, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.4637 - accuracy: 0.7201 - val_loss: 1.3614 - val_accuracy: 0.7536\n",
      "Epoch 20/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4479 - accuracy: 0.7295\n",
      "Epoch 00020: val_accuracy did not improve from 0.75360\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.4479 - accuracy: 0.7295 - val_loss: 1.3495 - val_accuracy: 0.7509\n",
      "Epoch 21/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4341 - accuracy: 0.7389\n",
      "Epoch 00021: val_accuracy improved from 0.75360 to 0.76260, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.4341 - accuracy: 0.7389 - val_loss: 1.3476 - val_accuracy: 0.7626\n",
      "Epoch 22/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4231 - accuracy: 0.7455\n",
      "Epoch 00022: val_accuracy improved from 0.76260 to 0.78680, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.4231 - accuracy: 0.7455 - val_loss: 1.2987 - val_accuracy: 0.7868\n",
      "Epoch 23/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4090 - accuracy: 0.7540\n",
      "Epoch 00023: val_accuracy did not improve from 0.78680\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.4090 - accuracy: 0.7540 - val_loss: 1.3272 - val_accuracy: 0.7691\n",
      "Epoch 24/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.4027 - accuracy: 0.7579\n",
      "Epoch 00024: val_accuracy improved from 0.78680 to 0.78990, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.4027 - accuracy: 0.7579 - val_loss: 1.3016 - val_accuracy: 0.7899\n",
      "Epoch 25/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3954 - accuracy: 0.7622\n",
      "Epoch 00025: val_accuracy improved from 0.78990 to 0.79010, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.3954 - accuracy: 0.7622 - val_loss: 1.2907 - val_accuracy: 0.7901\n",
      "Epoch 26/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3828 - accuracy: 0.7716\n",
      "Epoch 00026: val_accuracy improved from 0.79010 to 0.79080, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.3828 - accuracy: 0.7716 - val_loss: 1.2918 - val_accuracy: 0.7908\n",
      "Epoch 27/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.3726 - accuracy: 0.7768\n",
      "Epoch 00027: val_accuracy did not improve from 0.79080\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.3726 - accuracy: 0.7768 - val_loss: 1.3003 - val_accuracy: 0.7874\n",
      "Epoch 28/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.3657 - accuracy: 0.7830\n",
      "Epoch 00028: val_accuracy improved from 0.79080 to 0.80340, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.3657 - accuracy: 0.7830 - val_loss: 1.2744 - val_accuracy: 0.8034\n",
      "Epoch 29/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3590 - accuracy: 0.7851\n",
      "Epoch 00029: val_accuracy did not improve from 0.80340\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.3590 - accuracy: 0.7851 - val_loss: 1.2753 - val_accuracy: 0.7974\n",
      "Epoch 30/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.3495 - accuracy: 0.7916\n",
      "Epoch 00030: val_accuracy improved from 0.80340 to 0.80570, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.3495 - accuracy: 0.7916 - val_loss: 1.2800 - val_accuracy: 0.8057\n",
      "Epoch 31/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3443 - accuracy: 0.7939\n",
      "Epoch 00031: val_accuracy did not improve from 0.80570\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.3443 - accuracy: 0.7939 - val_loss: 1.2740 - val_accuracy: 0.7949\n",
      "Epoch 32/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3392 - accuracy: 0.7980\n",
      "Epoch 00032: val_accuracy did not improve from 0.80570\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.3392 - accuracy: 0.7980 - val_loss: 1.2698 - val_accuracy: 0.7993\n",
      "Epoch 33/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3310 - accuracy: 0.8027\n",
      "Epoch 00033: val_accuracy improved from 0.80570 to 0.81770, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.3310 - accuracy: 0.8027 - val_loss: 1.2445 - val_accuracy: 0.8177\n",
      "Epoch 34/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.3220 - accuracy: 0.8055\n",
      "Epoch 00034: val_accuracy did not improve from 0.81770\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.3220 - accuracy: 0.8055 - val_loss: 1.2483 - val_accuracy: 0.8132\n",
      "Epoch 35/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3161 - accuracy: 0.8117\n",
      "Epoch 00035: val_accuracy did not improve from 0.81770\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.3161 - accuracy: 0.8117 - val_loss: 1.2571 - val_accuracy: 0.8090\n",
      "Epoch 36/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3104 - accuracy: 0.8152\n",
      "Epoch 00036: val_accuracy did not improve from 0.81770\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.3104 - accuracy: 0.8152 - val_loss: 1.2657 - val_accuracy: 0.8071\n",
      "Epoch 37/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.3060 - accuracy: 0.8188\n",
      "Epoch 00037: val_accuracy did not improve from 0.81770\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.3060 - accuracy: 0.8188 - val_loss: 1.2462 - val_accuracy: 0.8161\n",
      "Epoch 38/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.3009 - accuracy: 0.8202\n",
      "Epoch 00038: val_accuracy did not improve from 0.81770\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.3009 - accuracy: 0.8202 - val_loss: 1.2470 - val_accuracy: 0.8139\n",
      "Epoch 39/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2932 - accuracy: 0.8231\n",
      "Epoch 00039: val_accuracy improved from 0.81770 to 0.82990, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.2932 - accuracy: 0.8231 - val_loss: 1.2208 - val_accuracy: 0.8299\n",
      "Epoch 40/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2892 - accuracy: 0.8273\n",
      "Epoch 00040: val_accuracy did not improve from 0.82990\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.2892 - accuracy: 0.8273 - val_loss: 1.2268 - val_accuracy: 0.8251\n",
      "Epoch 41/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2845 - accuracy: 0.8271\n",
      "Epoch 00041: val_accuracy improved from 0.82990 to 0.83460, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2845 - accuracy: 0.8271 - val_loss: 1.2123 - val_accuracy: 0.8346\n",
      "Epoch 42/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2805 - accuracy: 0.8333\n",
      "Epoch 00042: val_accuracy improved from 0.83460 to 0.83850, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2805 - accuracy: 0.8333 - val_loss: 1.2068 - val_accuracy: 0.8385\n",
      "Epoch 43/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2767 - accuracy: 0.8343\n",
      "Epoch 00043: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.2767 - accuracy: 0.8343 - val_loss: 1.2023 - val_accuracy: 0.8376\n",
      "Epoch 44/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2725 - accuracy: 0.8379\n",
      "Epoch 00044: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2725 - accuracy: 0.8378 - val_loss: 1.2274 - val_accuracy: 0.8259\n",
      "Epoch 45/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2676 - accuracy: 0.8410\n",
      "Epoch 00045: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2677 - accuracy: 0.8410 - val_loss: 1.2165 - val_accuracy: 0.8305\n",
      "Epoch 46/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2638 - accuracy: 0.8418\n",
      "Epoch 00046: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2638 - accuracy: 0.8418 - val_loss: 1.2296 - val_accuracy: 0.8227\n",
      "Epoch 47/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2588 - accuracy: 0.8446\n",
      "Epoch 00047: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.2588 - accuracy: 0.8446 - val_loss: 1.2188 - val_accuracy: 0.8261\n",
      "Epoch 48/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2558 - accuracy: 0.8468\n",
      "Epoch 00048: val_accuracy did not improve from 0.83850\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.2558 - accuracy: 0.8468 - val_loss: 1.2227 - val_accuracy: 0.8260\n",
      "Epoch 49/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2536 - accuracy: 0.8487\n",
      "Epoch 00049: val_accuracy improved from 0.83850 to 0.84350, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 381s 61ms/step - loss: 1.2536 - accuracy: 0.8487 - val_loss: 1.1977 - val_accuracy: 0.8435\n",
      "Epoch 50/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2467 - accuracy: 0.8515\n",
      "Epoch 00050: val_accuracy did not improve from 0.84350\n",
      "6250/6250 [==============================] - 359s 57ms/step - loss: 1.2467 - accuracy: 0.8515 - val_loss: 1.2054 - val_accuracy: 0.8386\n",
      "Epoch 51/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2449 - accuracy: 0.8526\n",
      "Epoch 00051: val_accuracy did not improve from 0.84350\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.2449 - accuracy: 0.8526 - val_loss: 1.2057 - val_accuracy: 0.8361\n",
      "Epoch 52/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2401 - accuracy: 0.8545\n",
      "Epoch 00052: val_accuracy did not improve from 0.84350\n",
      "6250/6250 [==============================] - 335s 54ms/step - loss: 1.2401 - accuracy: 0.8545 - val_loss: 1.1941 - val_accuracy: 0.8404\n",
      "Epoch 53/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2364 - accuracy: 0.8560\n",
      "Epoch 00053: val_accuracy improved from 0.84350 to 0.84720, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.2364 - accuracy: 0.8560 - val_loss: 1.1869 - val_accuracy: 0.8472\n",
      "Epoch 54/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2356 - accuracy: 0.8566\n",
      "Epoch 00054: val_accuracy did not improve from 0.84720\n",
      "6250/6250 [==============================] - 358s 57ms/step - loss: 1.2356 - accuracy: 0.8566 - val_loss: 1.1924 - val_accuracy: 0.8425\n",
      "Epoch 55/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2306 - accuracy: 0.8605\n",
      "Epoch 00055: val_accuracy improved from 0.84720 to 0.85440, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 356s 57ms/step - loss: 1.2306 - accuracy: 0.8605 - val_loss: 1.1732 - val_accuracy: 0.8544\n",
      "Epoch 56/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2300 - accuracy: 0.8618\n",
      "Epoch 00056: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.2300 - accuracy: 0.8618 - val_loss: 1.1925 - val_accuracy: 0.8428\n",
      "Epoch 57/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2252 - accuracy: 0.8642\n",
      "Epoch 00057: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 14901s 2s/step - loss: 1.2252 - accuracy: 0.8642 - val_loss: 1.1940 - val_accuracy: 0.8423\n",
      "Epoch 58/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2223 - accuracy: 0.8643\n",
      "Epoch 00058: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 328s 52ms/step - loss: 1.2222 - accuracy: 0.8643 - val_loss: 1.1974 - val_accuracy: 0.8454\n",
      "Epoch 59/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2188 - accuracy: 0.8669\n",
      "Epoch 00059: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 329s 53ms/step - loss: 1.2188 - accuracy: 0.8669 - val_loss: 1.2000 - val_accuracy: 0.8450\n",
      "Epoch 60/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2165 - accuracy: 0.8689\n",
      "Epoch 00060: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 331s 53ms/step - loss: 1.2165 - accuracy: 0.8689 - val_loss: 1.1942 - val_accuracy: 0.8410\n",
      "Epoch 61/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2139 - accuracy: 0.8710\n",
      "Epoch 00061: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.2139 - accuracy: 0.8710 - val_loss: 1.1809 - val_accuracy: 0.8481\n",
      "Epoch 62/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2149 - accuracy: 0.8703\n",
      "Epoch 00062: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 331s 53ms/step - loss: 1.2149 - accuracy: 0.8703 - val_loss: 1.1854 - val_accuracy: 0.8486\n",
      "Epoch 63/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.2075 - accuracy: 0.8737\n",
      "Epoch 00063: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 331s 53ms/step - loss: 1.2075 - accuracy: 0.8737 - val_loss: 1.1746 - val_accuracy: 0.8506\n",
      "Epoch 64/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2053 - accuracy: 0.8754\n",
      "Epoch 00064: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.2053 - accuracy: 0.8754 - val_loss: 1.1796 - val_accuracy: 0.8490\n",
      "Epoch 65/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2064 - accuracy: 0.8731\n",
      "Epoch 00065: val_accuracy did not improve from 0.85440\n",
      "6250/6250 [==============================] - 331s 53ms/step - loss: 1.2064 - accuracy: 0.8731 - val_loss: 1.1859 - val_accuracy: 0.8518\n",
      "Epoch 66/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.2006 - accuracy: 0.8777\n",
      "Epoch 00066: val_accuracy improved from 0.85440 to 0.85520, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.2006 - accuracy: 0.8777 - val_loss: 1.1743 - val_accuracy: 0.8552\n",
      "Epoch 67/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1934 - accuracy: 0.8811\n",
      "Epoch 00067: val_accuracy improved from 0.85520 to 0.85750, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.1934 - accuracy: 0.8811 - val_loss: 1.1751 - val_accuracy: 0.8575\n",
      "Epoch 68/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1942 - accuracy: 0.8805\n",
      "Epoch 00068: val_accuracy improved from 0.85750 to 0.85760, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.1943 - accuracy: 0.8805 - val_loss: 1.1652 - val_accuracy: 0.8576\n",
      "Epoch 69/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1909 - accuracy: 0.8808\n",
      "Epoch 00069: val_accuracy did not improve from 0.85760\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.1909 - accuracy: 0.8808 - val_loss: 1.1757 - val_accuracy: 0.8562\n",
      "Epoch 70/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1906 - accuracy: 0.8819\n",
      "Epoch 00070: val_accuracy improved from 0.85760 to 0.86090, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.1906 - accuracy: 0.8819 - val_loss: 1.1664 - val_accuracy: 0.8609\n",
      "Epoch 71/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1869 - accuracy: 0.8836\n",
      "Epoch 00071: val_accuracy did not improve from 0.86090\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.1869 - accuracy: 0.8836 - val_loss: 1.1678 - val_accuracy: 0.8554\n",
      "Epoch 72/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1873 - accuracy: 0.8845\n",
      "Epoch 00072: val_accuracy improved from 0.86090 to 0.86530, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.1873 - accuracy: 0.8844 - val_loss: 1.1621 - val_accuracy: 0.8653\n",
      "Epoch 73/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1830 - accuracy: 0.8856\n",
      "Epoch 00073: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.1830 - accuracy: 0.8856 - val_loss: 1.1707 - val_accuracy: 0.8566\n",
      "Epoch 74/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1792 - accuracy: 0.8876\n",
      "Epoch 00074: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 427s 68ms/step - loss: 1.1792 - accuracy: 0.8876 - val_loss: 1.1716 - val_accuracy: 0.8559\n",
      "Epoch 75/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1784 - accuracy: 0.8888\n",
      "Epoch 00075: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 420s 67ms/step - loss: 1.1784 - accuracy: 0.8888 - val_loss: 1.1620 - val_accuracy: 0.8611\n",
      "Epoch 76/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1742 - accuracy: 0.8921\n",
      "Epoch 00076: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 415s 66ms/step - loss: 1.1742 - accuracy: 0.8921 - val_loss: 1.1597 - val_accuracy: 0.8639\n",
      "Epoch 77/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1783 - accuracy: 0.8898\n",
      "Epoch 00077: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 283s 45ms/step - loss: 1.1783 - accuracy: 0.8898 - val_loss: 1.1617 - val_accuracy: 0.8593\n",
      "Epoch 78/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1733 - accuracy: 0.8918\n",
      "Epoch 00078: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 333s 53ms/step - loss: 1.1733 - accuracy: 0.8918 - val_loss: 1.1665 - val_accuracy: 0.8587\n",
      "Epoch 79/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1717 - accuracy: 0.8932\n",
      "Epoch 00079: val_accuracy did not improve from 0.86530\n",
      "6250/6250 [==============================] - 330s 53ms/step - loss: 1.1717 - accuracy: 0.8932 - val_loss: 1.1534 - val_accuracy: 0.8653\n",
      "Epoch 80/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1675 - accuracy: 0.8948\n",
      "Epoch 00080: val_accuracy improved from 0.86530 to 0.86590, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1675 - accuracy: 0.8948 - val_loss: 1.1593 - val_accuracy: 0.8659\n",
      "Epoch 81/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1665 - accuracy: 0.8951\n",
      "Epoch 00081: val_accuracy did not improve from 0.86590\n",
      "6250/6250 [==============================] - 348s 56ms/step - loss: 1.1665 - accuracy: 0.8951 - val_loss: 1.1603 - val_accuracy: 0.8627\n",
      "Epoch 82/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1634 - accuracy: 0.8974\n",
      "Epoch 00082: val_accuracy did not improve from 0.86590\n",
      "6250/6250 [==============================] - 329s 53ms/step - loss: 1.1634 - accuracy: 0.8974 - val_loss: 1.1480 - val_accuracy: 0.8637\n",
      "Epoch 83/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1595 - accuracy: 0.8975\n",
      "Epoch 00083: val_accuracy did not improve from 0.86590\n",
      "6250/6250 [==============================] - 328s 53ms/step - loss: 1.1595 - accuracy: 0.8975 - val_loss: 1.1660 - val_accuracy: 0.8563\n",
      "Epoch 84/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1571 - accuracy: 0.8990\n",
      "Epoch 00084: val_accuracy did not improve from 0.86590\n",
      "6250/6250 [==============================] - 327s 52ms/step - loss: 1.1571 - accuracy: 0.8990 - val_loss: 1.1594 - val_accuracy: 0.8643\n",
      "Epoch 85/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1561 - accuracy: 0.9000\n",
      "Epoch 00085: val_accuracy improved from 0.86590 to 0.87220, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 329s 53ms/step - loss: 1.1561 - accuracy: 0.9000 - val_loss: 1.1401 - val_accuracy: 0.8722\n",
      "Epoch 86/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1510 - accuracy: 0.9014\n",
      "Epoch 00086: val_accuracy did not improve from 0.87220\n",
      "6250/6250 [==============================] - 327s 52ms/step - loss: 1.1510 - accuracy: 0.9014 - val_loss: 1.1495 - val_accuracy: 0.8657\n",
      "Epoch 87/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1475 - accuracy: 0.9039\n",
      "Epoch 00087: val_accuracy improved from 0.87220 to 0.87460, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 327s 52ms/step - loss: 1.1475 - accuracy: 0.9039 - val_loss: 1.1399 - val_accuracy: 0.8746\n",
      "Epoch 88/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1476 - accuracy: 0.9038\n",
      "Epoch 00088: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 329s 53ms/step - loss: 1.1476 - accuracy: 0.9038 - val_loss: 1.1558 - val_accuracy: 0.8612\n",
      "Epoch 89/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1480 - accuracy: 0.9041\n",
      "Epoch 00089: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 327s 52ms/step - loss: 1.1480 - accuracy: 0.9041 - val_loss: 1.1594 - val_accuracy: 0.8598\n",
      "Epoch 90/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1435 - accuracy: 0.9067\n",
      "Epoch 00090: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 332s 53ms/step - loss: 1.1435 - accuracy: 0.9067 - val_loss: 1.1588 - val_accuracy: 0.8637\n",
      "Epoch 91/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1435 - accuracy: 0.9062\n",
      "Epoch 00091: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 327s 52ms/step - loss: 1.1435 - accuracy: 0.9061 - val_loss: 1.1542 - val_accuracy: 0.8650\n",
      "Epoch 92/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1402 - accuracy: 0.9062\n",
      "Epoch 00092: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 334s 53ms/step - loss: 1.1402 - accuracy: 0.9062 - val_loss: 1.1529 - val_accuracy: 0.8622\n",
      "Epoch 93/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1384 - accuracy: 0.9091\n",
      "Epoch 00093: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1384 - accuracy: 0.9091 - val_loss: 1.1626 - val_accuracy: 0.8610\n",
      "Epoch 94/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1399 - accuracy: 0.9095\n",
      "Epoch 00094: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1399 - accuracy: 0.9095 - val_loss: 1.1636 - val_accuracy: 0.8592\n",
      "Epoch 95/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1376 - accuracy: 0.9095\n",
      "Epoch 00095: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1376 - accuracy: 0.9095 - val_loss: 1.1400 - val_accuracy: 0.8737\n",
      "Epoch 96/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1364 - accuracy: 0.9110\n",
      "Epoch 00096: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.1364 - accuracy: 0.9110 - val_loss: 1.1528 - val_accuracy: 0.8675\n",
      "Epoch 97/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1338 - accuracy: 0.9108\n",
      "Epoch 00097: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1338 - accuracy: 0.9108 - val_loss: 1.1498 - val_accuracy: 0.8662\n",
      "Epoch 98/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1314 - accuracy: 0.9123\n",
      "Epoch 00098: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1313 - accuracy: 0.9123 - val_loss: 1.1501 - val_accuracy: 0.8686\n",
      "Epoch 99/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1300 - accuracy: 0.9131\n",
      "Epoch 00099: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1300 - accuracy: 0.9131 - val_loss: 1.1397 - val_accuracy: 0.8709\n",
      "Epoch 100/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1306 - accuracy: 0.9127\n",
      "Epoch 00100: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1306 - accuracy: 0.9127 - val_loss: 1.1520 - val_accuracy: 0.8686\n",
      "Epoch 101/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1291 - accuracy: 0.9137\n",
      "Epoch 00101: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1291 - accuracy: 0.9137 - val_loss: 1.1507 - val_accuracy: 0.8640\n",
      "Epoch 102/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1269 - accuracy: 0.9139\n",
      "Epoch 00102: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1269 - accuracy: 0.9139 - val_loss: 1.1499 - val_accuracy: 0.8667\n",
      "Epoch 103/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1249 - accuracy: 0.9154\n",
      "Epoch 00103: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 337s 54ms/step - loss: 1.1249 - accuracy: 0.9154 - val_loss: 1.1420 - val_accuracy: 0.8670\n",
      "Epoch 104/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1237 - accuracy: 0.9173\n",
      "Epoch 00104: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1237 - accuracy: 0.9173 - val_loss: 1.1497 - val_accuracy: 0.8679\n",
      "Epoch 105/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1213 - accuracy: 0.9178\n",
      "Epoch 00105: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1213 - accuracy: 0.9178 - val_loss: 1.1315 - val_accuracy: 0.8743\n",
      "Epoch 106/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1228 - accuracy: 0.9162\n",
      "Epoch 00106: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1228 - accuracy: 0.9162 - val_loss: 1.1546 - val_accuracy: 0.8618\n",
      "Epoch 107/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1181 - accuracy: 0.9196\n",
      "Epoch 00107: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1181 - accuracy: 0.9196 - val_loss: 1.1396 - val_accuracy: 0.8666\n",
      "Epoch 108/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1191 - accuracy: 0.9180\n",
      "Epoch 00108: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1191 - accuracy: 0.9180 - val_loss: 1.1497 - val_accuracy: 0.8680\n",
      "Epoch 109/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1211 - accuracy: 0.9194\n",
      "Epoch 00109: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1211 - accuracy: 0.9194 - val_loss: 1.1439 - val_accuracy: 0.8726\n",
      "Epoch 110/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1141 - accuracy: 0.9230\n",
      "Epoch 00110: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1141 - accuracy: 0.9230 - val_loss: 1.1503 - val_accuracy: 0.8665\n",
      "Epoch 111/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1156 - accuracy: 0.9209\n",
      "Epoch 00111: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.1156 - accuracy: 0.9209 - val_loss: 1.1323 - val_accuracy: 0.8719\n",
      "Epoch 112/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1138 - accuracy: 0.9219\n",
      "Epoch 00112: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 336s 54ms/step - loss: 1.1138 - accuracy: 0.9219 - val_loss: 1.1414 - val_accuracy: 0.8717\n",
      "Epoch 113/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1134 - accuracy: 0.9205\n",
      "Epoch 00113: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1134 - accuracy: 0.9205 - val_loss: 1.1517 - val_accuracy: 0.8665\n",
      "Epoch 114/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1146 - accuracy: 0.9212\n",
      "Epoch 00114: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.1146 - accuracy: 0.9212 - val_loss: 1.1489 - val_accuracy: 0.8693\n",
      "Epoch 115/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1085 - accuracy: 0.9250\n",
      "Epoch 00115: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1085 - accuracy: 0.9250 - val_loss: 1.1394 - val_accuracy: 0.8710\n",
      "Epoch 116/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1104 - accuracy: 0.9239\n",
      "Epoch 00116: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.1104 - accuracy: 0.9239 - val_loss: 1.1381 - val_accuracy: 0.8726\n",
      "Epoch 117/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1113 - accuracy: 0.9230\n",
      "Epoch 00117: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.1113 - accuracy: 0.9230 - val_loss: 1.1581 - val_accuracy: 0.8634\n",
      "Epoch 118/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1089 - accuracy: 0.9247\n",
      "Epoch 00118: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1089 - accuracy: 0.9247 - val_loss: 1.1378 - val_accuracy: 0.8727\n",
      "Epoch 119/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.1065 - accuracy: 0.9256\n",
      "Epoch 00119: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1065 - accuracy: 0.9256 - val_loss: 1.1374 - val_accuracy: 0.8738\n",
      "Epoch 120/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1061 - accuracy: 0.9259\n",
      "Epoch 00120: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1061 - accuracy: 0.9259 - val_loss: 1.1431 - val_accuracy: 0.8668\n",
      "Epoch 121/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1061 - accuracy: 0.9254\n",
      "Epoch 00121: val_accuracy did not improve from 0.87460\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1061 - accuracy: 0.9254 - val_loss: 1.1367 - val_accuracy: 0.8697\n",
      "Epoch 122/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1047 - accuracy: 0.9260\n",
      "Epoch 00122: val_accuracy improved from 0.87460 to 0.87710, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.1047 - accuracy: 0.9260 - val_loss: 1.1253 - val_accuracy: 0.8771\n",
      "Epoch 123/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1029 - accuracy: 0.9271\n",
      "Epoch 00123: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.1029 - accuracy: 0.9271 - val_loss: 1.1423 - val_accuracy: 0.8733\n",
      "Epoch 124/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.1005 - accuracy: 0.9295\n",
      "Epoch 00124: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.1005 - accuracy: 0.9295 - val_loss: 1.1420 - val_accuracy: 0.8696\n",
      "Epoch 125/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0994 - accuracy: 0.9284\n",
      "Epoch 00125: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0994 - accuracy: 0.9284 - val_loss: 1.1517 - val_accuracy: 0.8684\n",
      "Epoch 126/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0995 - accuracy: 0.9295\n",
      "Epoch 00126: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0995 - accuracy: 0.9295 - val_loss: 1.1422 - val_accuracy: 0.8709\n",
      "Epoch 127/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0964 - accuracy: 0.9303\n",
      "Epoch 00127: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0964 - accuracy: 0.9303 - val_loss: 1.1542 - val_accuracy: 0.8637\n",
      "Epoch 128/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0992 - accuracy: 0.9297\n",
      "Epoch 00128: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0992 - accuracy: 0.9297 - val_loss: 1.1384 - val_accuracy: 0.8737\n",
      "Epoch 129/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0975 - accuracy: 0.9300\n",
      "Epoch 00129: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.0975 - accuracy: 0.9300 - val_loss: 1.1459 - val_accuracy: 0.8687\n",
      "Epoch 130/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0968 - accuracy: 0.9303\n",
      "Epoch 00130: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0968 - accuracy: 0.9303 - val_loss: 1.1361 - val_accuracy: 0.8733\n",
      "Epoch 131/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0949 - accuracy: 0.9315\n",
      "Epoch 00131: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0950 - accuracy: 0.9315 - val_loss: 1.1286 - val_accuracy: 0.8742\n",
      "Epoch 132/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0963 - accuracy: 0.9307\n",
      "Epoch 00132: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0963 - accuracy: 0.9307 - val_loss: 1.1272 - val_accuracy: 0.8758\n",
      "Epoch 133/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0941 - accuracy: 0.9323\n",
      "Epoch 00133: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0941 - accuracy: 0.9323 - val_loss: 1.1414 - val_accuracy: 0.8718\n",
      "Epoch 134/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0922 - accuracy: 0.9324\n",
      "Epoch 00134: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0922 - accuracy: 0.9324 - val_loss: 1.1284 - val_accuracy: 0.8754\n",
      "Epoch 135/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0903 - accuracy: 0.9347\n",
      "Epoch 00135: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0903 - accuracy: 0.9347 - val_loss: 1.1331 - val_accuracy: 0.8733\n",
      "Epoch 136/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.9328\n",
      "Epoch 00136: val_accuracy did not improve from 0.87710\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0925 - accuracy: 0.9328 - val_loss: 1.1262 - val_accuracy: 0.8764\n",
      "Epoch 137/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0895 - accuracy: 0.9344\n",
      "Epoch 00137: val_accuracy improved from 0.87710 to 0.87960, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0895 - accuracy: 0.9344 - val_loss: 1.1279 - val_accuracy: 0.8796\n",
      "Epoch 138/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0891 - accuracy: 0.9347\n",
      "Epoch 00138: val_accuracy did not improve from 0.87960\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0891 - accuracy: 0.9347 - val_loss: 1.1236 - val_accuracy: 0.8789\n",
      "Epoch 139/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0887 - accuracy: 0.9355\n",
      "Epoch 00139: val_accuracy did not improve from 0.87960\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0887 - accuracy: 0.9355 - val_loss: 1.1403 - val_accuracy: 0.8688\n",
      "Epoch 140/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0869 - accuracy: 0.9361\n",
      "Epoch 00140: val_accuracy did not improve from 0.87960\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0869 - accuracy: 0.9361 - val_loss: 1.1334 - val_accuracy: 0.8741\n",
      "Epoch 141/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0884 - accuracy: 0.9354\n",
      "Epoch 00141: val_accuracy did not improve from 0.87960\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0884 - accuracy: 0.9354 - val_loss: 1.1307 - val_accuracy: 0.8771\n",
      "Epoch 142/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0849 - accuracy: 0.9359\n",
      "Epoch 00142: val_accuracy did not improve from 0.87960\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0849 - accuracy: 0.9359 - val_loss: 1.1381 - val_accuracy: 0.8758\n",
      "Epoch 143/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0851 - accuracy: 0.9362\n",
      "Epoch 00143: val_accuracy improved from 0.87960 to 0.88130, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0851 - accuracy: 0.9362 - val_loss: 1.1250 - val_accuracy: 0.8813\n",
      "Epoch 144/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.9381\n",
      "Epoch 00144: val_accuracy did not improve from 0.88130\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0821 - accuracy: 0.9381 - val_loss: 1.1255 - val_accuracy: 0.8781\n",
      "Epoch 145/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0830 - accuracy: 0.9379\n",
      "Epoch 00145: val_accuracy did not improve from 0.88130\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0830 - accuracy: 0.9379 - val_loss: 1.1335 - val_accuracy: 0.8745\n",
      "Epoch 146/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0825 - accuracy: 0.9374\n",
      "Epoch 00146: val_accuracy did not improve from 0.88130\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0825 - accuracy: 0.9374 - val_loss: 1.1208 - val_accuracy: 0.8785\n",
      "Epoch 147/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0824 - accuracy: 0.9365\n",
      "Epoch 00147: val_accuracy did not improve from 0.88130\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0824 - accuracy: 0.9365 - val_loss: 1.1243 - val_accuracy: 0.8793\n",
      "Epoch 148/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0809 - accuracy: 0.9387\n",
      "Epoch 00148: val_accuracy improved from 0.88130 to 0.88220, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0809 - accuracy: 0.9387 - val_loss: 1.1257 - val_accuracy: 0.8822\n",
      "Epoch 149/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0806 - accuracy: 0.9381\n",
      "Epoch 00149: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0806 - accuracy: 0.9381 - val_loss: 1.1242 - val_accuracy: 0.8766\n",
      "Epoch 150/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0796 - accuracy: 0.9386\n",
      "Epoch 00150: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0796 - accuracy: 0.9386 - val_loss: 1.1316 - val_accuracy: 0.8747\n",
      "Epoch 151/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0774 - accuracy: 0.9395\n",
      "Epoch 00151: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0774 - accuracy: 0.9395 - val_loss: 1.1382 - val_accuracy: 0.8728\n",
      "Epoch 152/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.9400\n",
      "Epoch 00152: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0762 - accuracy: 0.9400 - val_loss: 1.1313 - val_accuracy: 0.8791\n",
      "Epoch 153/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0791 - accuracy: 0.9387\n",
      "Epoch 00153: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0791 - accuracy: 0.9387 - val_loss: 1.1326 - val_accuracy: 0.8751\n",
      "Epoch 154/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0772 - accuracy: 0.9417\n",
      "Epoch 00154: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0772 - accuracy: 0.9417 - val_loss: 1.1352 - val_accuracy: 0.8715\n",
      "Epoch 155/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0786 - accuracy: 0.9396\n",
      "Epoch 00155: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0786 - accuracy: 0.9396 - val_loss: 1.1243 - val_accuracy: 0.8784\n",
      "Epoch 156/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0759 - accuracy: 0.9412\n",
      "Epoch 00156: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0759 - accuracy: 0.9412 - val_loss: 1.1363 - val_accuracy: 0.8745\n",
      "Epoch 157/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.9409\n",
      "Epoch 00157: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0744 - accuracy: 0.9409 - val_loss: 1.1341 - val_accuracy: 0.8769\n",
      "Epoch 158/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0754 - accuracy: 0.9421\n",
      "Epoch 00158: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0754 - accuracy: 0.9421 - val_loss: 1.1461 - val_accuracy: 0.8719\n",
      "Epoch 159/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0749 - accuracy: 0.9418\n",
      "Epoch 00159: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 341s 54ms/step - loss: 1.0749 - accuracy: 0.9418 - val_loss: 1.1366 - val_accuracy: 0.8754\n",
      "Epoch 160/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.9431\n",
      "Epoch 00160: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0736 - accuracy: 0.9431 - val_loss: 1.1441 - val_accuracy: 0.8698\n",
      "Epoch 161/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0718 - accuracy: 0.9426\n",
      "Epoch 00161: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0718 - accuracy: 0.9426 - val_loss: 1.1368 - val_accuracy: 0.8771\n",
      "Epoch 162/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0722 - accuracy: 0.9431\n",
      "Epoch 00162: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0722 - accuracy: 0.9430 - val_loss: 1.1180 - val_accuracy: 0.8802\n",
      "Epoch 163/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0698 - accuracy: 0.9450\n",
      "Epoch 00163: val_accuracy did not improve from 0.88220\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0698 - accuracy: 0.9450 - val_loss: 1.1287 - val_accuracy: 0.8759\n",
      "Epoch 164/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0707 - accuracy: 0.9446\n",
      "Epoch 00164: val_accuracy improved from 0.88220 to 0.88260, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0707 - accuracy: 0.9446 - val_loss: 1.1211 - val_accuracy: 0.8826\n",
      "Epoch 165/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0682 - accuracy: 0.9453\n",
      "Epoch 00165: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0682 - accuracy: 0.9453 - val_loss: 1.1159 - val_accuracy: 0.8826\n",
      "Epoch 166/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0687 - accuracy: 0.9458\n",
      "Epoch 00166: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0687 - accuracy: 0.9458 - val_loss: 1.1212 - val_accuracy: 0.8799\n",
      "Epoch 167/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0700 - accuracy: 0.9439\n",
      "Epoch 00167: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0700 - accuracy: 0.9439 - val_loss: 1.1297 - val_accuracy: 0.8763\n",
      "Epoch 168/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0654 - accuracy: 0.9461\n",
      "Epoch 00168: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0654 - accuracy: 0.9461 - val_loss: 1.1233 - val_accuracy: 0.8774\n",
      "Epoch 169/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0679 - accuracy: 0.9449\n",
      "Epoch 00169: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0679 - accuracy: 0.9449 - val_loss: 1.1244 - val_accuracy: 0.8790\n",
      "Epoch 170/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0657 - accuracy: 0.9450\n",
      "Epoch 00170: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0657 - accuracy: 0.9449 - val_loss: 1.1286 - val_accuracy: 0.8770\n",
      "Epoch 171/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0652 - accuracy: 0.9467\n",
      "Epoch 00171: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0652 - accuracy: 0.9467 - val_loss: 1.1261 - val_accuracy: 0.8780\n",
      "Epoch 172/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0617 - accuracy: 0.9484\n",
      "Epoch 00172: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0617 - accuracy: 0.9484 - val_loss: 1.1218 - val_accuracy: 0.8773\n",
      "Epoch 173/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0620 - accuracy: 0.9472\n",
      "Epoch 00173: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0620 - accuracy: 0.9472 - val_loss: 1.1275 - val_accuracy: 0.8806\n",
      "Epoch 174/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0635 - accuracy: 0.9459\n",
      "Epoch 00174: val_accuracy did not improve from 0.88260\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0635 - accuracy: 0.9459 - val_loss: 1.1227 - val_accuracy: 0.8793\n",
      "Epoch 175/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0624 - accuracy: 0.9473\n",
      "Epoch 00175: val_accuracy improved from 0.88260 to 0.88450, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0624 - accuracy: 0.9473 - val_loss: 1.1120 - val_accuracy: 0.8845\n",
      "Epoch 176/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0635 - accuracy: 0.9460\n",
      "Epoch 00176: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0635 - accuracy: 0.9460 - val_loss: 1.1225 - val_accuracy: 0.8810\n",
      "Epoch 177/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0599 - accuracy: 0.9484\n",
      "Epoch 00177: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0599 - accuracy: 0.9484 - val_loss: 1.1448 - val_accuracy: 0.8673\n",
      "Epoch 178/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0608 - accuracy: 0.9483\n",
      "Epoch 00178: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0608 - accuracy: 0.9483 - val_loss: 1.1531 - val_accuracy: 0.8681\n",
      "Epoch 179/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0597 - accuracy: 0.9484\n",
      "Epoch 00179: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 345s 55ms/step - loss: 1.0597 - accuracy: 0.9484 - val_loss: 1.1300 - val_accuracy: 0.8759\n",
      "Epoch 180/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0614 - accuracy: 0.9479\n",
      "Epoch 00180: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0614 - accuracy: 0.9479 - val_loss: 1.1329 - val_accuracy: 0.8784\n",
      "Epoch 181/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0581 - accuracy: 0.9494\n",
      "Epoch 00181: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0581 - accuracy: 0.9494 - val_loss: 1.1298 - val_accuracy: 0.8758\n",
      "Epoch 182/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0561 - accuracy: 0.9514\n",
      "Epoch 00182: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0561 - accuracy: 0.9514 - val_loss: 1.1400 - val_accuracy: 0.8728\n",
      "Epoch 183/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0574 - accuracy: 0.9493\n",
      "Epoch 00183: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0574 - accuracy: 0.9493 - val_loss: 1.1229 - val_accuracy: 0.8843\n",
      "Epoch 184/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0576 - accuracy: 0.9506\n",
      "Epoch 00184: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0576 - accuracy: 0.9506 - val_loss: 1.1281 - val_accuracy: 0.8764\n",
      "Epoch 185/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0557 - accuracy: 0.9515\n",
      "Epoch 00185: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 346s 55ms/step - loss: 1.0557 - accuracy: 0.9515 - val_loss: 1.1283 - val_accuracy: 0.8754\n",
      "Epoch 186/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0570 - accuracy: 0.9500\n",
      "Epoch 00186: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0570 - accuracy: 0.9500 - val_loss: 1.1258 - val_accuracy: 0.8768\n",
      "Epoch 187/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0561 - accuracy: 0.9505\n",
      "Epoch 00187: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0561 - accuracy: 0.9505 - val_loss: 1.1218 - val_accuracy: 0.8818\n",
      "Epoch 188/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0543 - accuracy: 0.9517\n",
      "Epoch 00188: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0544 - accuracy: 0.9517 - val_loss: 1.1255 - val_accuracy: 0.8810\n",
      "Epoch 189/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0525 - accuracy: 0.9519\n",
      "Epoch 00189: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0525 - accuracy: 0.9519 - val_loss: 1.1438 - val_accuracy: 0.8695\n",
      "Epoch 190/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0528 - accuracy: 0.9515\n",
      "Epoch 00190: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0528 - accuracy: 0.9515 - val_loss: 1.1214 - val_accuracy: 0.8810\n",
      "Epoch 191/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0491 - accuracy: 0.9531\n",
      "Epoch 00191: val_accuracy did not improve from 0.88450\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0491 - accuracy: 0.9531 - val_loss: 1.1384 - val_accuracy: 0.8756\n",
      "Epoch 192/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0538 - accuracy: 0.9514\n",
      "Epoch 00192: val_accuracy improved from 0.88450 to 0.88510, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0538 - accuracy: 0.9514 - val_loss: 1.1204 - val_accuracy: 0.8851\n",
      "Epoch 193/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.9514\n",
      "Epoch 00193: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0530 - accuracy: 0.9514 - val_loss: 1.1329 - val_accuracy: 0.8742\n",
      "Epoch 194/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0516 - accuracy: 0.9529\n",
      "Epoch 00194: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0516 - accuracy: 0.9529 - val_loss: 1.1342 - val_accuracy: 0.8774\n",
      "Epoch 195/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0518 - accuracy: 0.9528\n",
      "Epoch 00195: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0518 - accuracy: 0.9528 - val_loss: 1.1297 - val_accuracy: 0.8761\n",
      "Epoch 196/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0508 - accuracy: 0.9527\n",
      "Epoch 00196: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0508 - accuracy: 0.9527 - val_loss: 1.1317 - val_accuracy: 0.8801\n",
      "Epoch 197/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0499 - accuracy: 0.9534\n",
      "Epoch 00197: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0499 - accuracy: 0.9534 - val_loss: 1.1438 - val_accuracy: 0.8689\n",
      "Epoch 198/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0477 - accuracy: 0.9538\n",
      "Epoch 00198: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0477 - accuracy: 0.9538 - val_loss: 1.1303 - val_accuracy: 0.8756\n",
      "Epoch 199/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0490 - accuracy: 0.9536\n",
      "Epoch 00199: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0491 - accuracy: 0.9536 - val_loss: 1.1369 - val_accuracy: 0.8738\n",
      "Epoch 200/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0488 - accuracy: 0.9546\n",
      "Epoch 00200: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0488 - accuracy: 0.9546 - val_loss: 1.1189 - val_accuracy: 0.8823\n",
      "Epoch 201/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0465 - accuracy: 0.9554\n",
      "Epoch 00201: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0465 - accuracy: 0.9554 - val_loss: 1.1196 - val_accuracy: 0.8805\n",
      "Epoch 202/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0469 - accuracy: 0.9548\n",
      "Epoch 00202: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0469 - accuracy: 0.9548 - val_loss: 1.1397 - val_accuracy: 0.8751\n",
      "Epoch 203/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0459 - accuracy: 0.9566\n",
      "Epoch 00203: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0459 - accuracy: 0.9566 - val_loss: 1.1334 - val_accuracy: 0.8733\n",
      "Epoch 204/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0456 - accuracy: 0.9549\n",
      "Epoch 00204: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0456 - accuracy: 0.9549 - val_loss: 1.1224 - val_accuracy: 0.8819\n",
      "Epoch 205/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0460 - accuracy: 0.9543\n",
      "Epoch 00205: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 340s 54ms/step - loss: 1.0460 - accuracy: 0.9543 - val_loss: 1.1277 - val_accuracy: 0.8811\n",
      "Epoch 206/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0461 - accuracy: 0.9552\n",
      "Epoch 00206: val_accuracy did not improve from 0.88510\n",
      "6250/6250 [==============================] - 344s 55ms/step - loss: 1.0461 - accuracy: 0.9552 - val_loss: 1.1225 - val_accuracy: 0.8827\n",
      "Epoch 207/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0470 - accuracy: 0.9545\n",
      "Epoch 00207: val_accuracy improved from 0.88510 to 0.88850, saving model to models\\20231128-102227_resnet_18.h5\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0470 - accuracy: 0.9545 - val_loss: 1.1143 - val_accuracy: 0.8885\n",
      "Epoch 208/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0441 - accuracy: 0.9561\n",
      "Epoch 00208: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0441 - accuracy: 0.9561 - val_loss: 1.1180 - val_accuracy: 0.8843\n",
      "Epoch 209/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0426 - accuracy: 0.9578\n",
      "Epoch 00209: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0426 - accuracy: 0.9578 - val_loss: 1.1346 - val_accuracy: 0.8747\n",
      "Epoch 210/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0437 - accuracy: 0.9560\n",
      "Epoch 00210: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0437 - accuracy: 0.9560 - val_loss: 1.1179 - val_accuracy: 0.8802\n",
      "Epoch 211/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0445 - accuracy: 0.9559\n",
      "Epoch 00211: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0445 - accuracy: 0.9559 - val_loss: 1.1178 - val_accuracy: 0.8810\n",
      "Epoch 212/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0436 - accuracy: 0.9564\n",
      "Epoch 00212: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0436 - accuracy: 0.9564 - val_loss: 1.1370 - val_accuracy: 0.8768\n",
      "Epoch 213/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0439 - accuracy: 0.9559\n",
      "Epoch 00213: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0439 - accuracy: 0.9559 - val_loss: 1.1246 - val_accuracy: 0.8816\n",
      "Epoch 214/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0417 - accuracy: 0.9577\n",
      "Epoch 00214: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0417 - accuracy: 0.9577 - val_loss: 1.1187 - val_accuracy: 0.8838\n",
      "Epoch 215/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.9550\n",
      "Epoch 00215: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0447 - accuracy: 0.9549 - val_loss: 1.1250 - val_accuracy: 0.8804\n",
      "Epoch 216/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0423 - accuracy: 0.9569\n",
      "Epoch 00216: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0423 - accuracy: 0.9569 - val_loss: 1.1266 - val_accuracy: 0.8748\n",
      "Epoch 217/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0408 - accuracy: 0.9585\n",
      "Epoch 00217: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 341s 55ms/step - loss: 1.0408 - accuracy: 0.9585 - val_loss: 1.1324 - val_accuracy: 0.8735\n",
      "Epoch 218/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.9571\n",
      "Epoch 00218: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0423 - accuracy: 0.9571 - val_loss: 1.1303 - val_accuracy: 0.8812\n",
      "Epoch 219/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0393 - accuracy: 0.9583\n",
      "Epoch 00219: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 342s 55ms/step - loss: 1.0393 - accuracy: 0.9583 - val_loss: 1.1358 - val_accuracy: 0.8728\n",
      "Epoch 220/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0411 - accuracy: 0.9570\n",
      "Epoch 00220: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 344s 55ms/step - loss: 1.0411 - accuracy: 0.9570 - val_loss: 1.1227 - val_accuracy: 0.8800\n",
      "Epoch 221/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0398 - accuracy: 0.9582\n",
      "Epoch 00221: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 343s 55ms/step - loss: 1.0398 - accuracy: 0.9582 - val_loss: 1.1263 - val_accuracy: 0.8756\n",
      "Epoch 222/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0397 - accuracy: 0.9575\n",
      "Epoch 00222: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0397 - accuracy: 0.9575 - val_loss: 1.1186 - val_accuracy: 0.8821\n",
      "Epoch 223/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.9587\n",
      "Epoch 00223: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 338s 54ms/step - loss: 1.0381 - accuracy: 0.9587 - val_loss: 1.1273 - val_accuracy: 0.8768\n",
      "Epoch 224/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0393 - accuracy: 0.9595\n",
      "Epoch 00224: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 452s 72ms/step - loss: 1.0393 - accuracy: 0.9595 - val_loss: 1.1129 - val_accuracy: 0.8834\n",
      "Epoch 225/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.9607\n",
      "Epoch 00225: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 494s 79ms/step - loss: 1.0351 - accuracy: 0.9607 - val_loss: 1.1163 - val_accuracy: 0.8854\n",
      "Epoch 226/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.9590\n",
      "Epoch 00226: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 363s 58ms/step - loss: 1.0374 - accuracy: 0.9590 - val_loss: 1.1227 - val_accuracy: 0.8821\n",
      "Epoch 227/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0401 - accuracy: 0.9578\n",
      "Epoch 00227: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 346s 55ms/step - loss: 1.0401 - accuracy: 0.9577 - val_loss: 1.1178 - val_accuracy: 0.8824\n",
      "Epoch 228/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0384 - accuracy: 0.9586\n",
      "Epoch 00228: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 339s 54ms/step - loss: 1.0384 - accuracy: 0.9586 - val_loss: 1.1423 - val_accuracy: 0.8742\n",
      "Epoch 229/300\n",
      "6249/6250 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.9601\n",
      "Epoch 00229: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 360s 58ms/step - loss: 1.0364 - accuracy: 0.9601 - val_loss: 1.1190 - val_accuracy: 0.8822\n",
      "Epoch 230/300\n",
      "6250/6250 [==============================] - ETA: 0s - loss: 1.0380 - accuracy: 0.9591\n",
      "Epoch 00230: val_accuracy did not improve from 0.88850\n",
      "6250/6250 [==============================] - 341s 54ms/step - loss: 1.0380 - accuracy: 0.9591 - val_loss: 1.1195 - val_accuracy: 0.8790\n",
      "Epoch 231/300\n",
      "  58/6250 [..............................] - ETA: 6:03 - loss: 1.0211 - accuracy: 0.9591"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benoi\\Documents\\zz_UQAC\\Apprentissage Automatique pour Données Maasives\\TP_8INF919\\devoir_2.ipynb Cell 59\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run train and validation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m run_hist \u001b[39m=\u001b[39m train_test_model(model)\n",
      "\u001b[1;32mc:\\Users\\benoi\\Documents\\zz_UQAC\\Apprentissage Automatique pour Données Maasives\\TP_8INF919\\devoir_2.ipynb Cell 59\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtensorboard --logdir \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m log_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m run_hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(ds_train, validation_data\u001b[39m=\u001b[39;49mds_test,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                      epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                      callbacks\u001b[39m=\u001b[39;49m[checkpoint, tensorboard_callback])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benoi/Documents/zz_UQAC/Apprentissage%20Automatique%20pour%20Donn%C3%A9es%20Maasives/TP_8INF919/devoir_2.ipynb#Y106sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m run_hist\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:103\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_method_wrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    105\u001b[0m   \u001b[39m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m   \u001b[39mif\u001b[39;00m dc_context\u001b[39m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1093\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1087\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTraceContext\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1088\u001b[0m     graph_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1089\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1090\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1091\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size):\n\u001b[0;32m   1092\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1093\u001b[0m   tmp_logs \u001b[39m=\u001b[39m train_function(iterator)\n\u001b[0;32m   1094\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1095\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_filtered_call(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_filtered_call\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs, cancellation_manager\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[39m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[39m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   1844\u001b[0m       [t \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m nest\u001b[39m.\u001b[39;49mflatten((args, kwargs), expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1845\u001b[0m        \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(t, (ops\u001b[39m.\u001b[39;49mTensor,\n\u001b[0;32m   1846\u001b[0m                          resource_variable_ops\u001b[39m.\u001b[39;49mBaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m       captured_inputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m   1848\u001b[0m       cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1924\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    546\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    547\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    548\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    549\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    550\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    551\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\benoi\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tp-8inf919-aDMhdl4N-py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run train and validation\n",
    "run_hist = train_test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "test_loss, test_acc = model.evaluate(ds_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if auto_encode:\n",
    "    # Show input output for one image\n",
    "    for image, label in ds_test.take(1):\n",
    "        pred = model.predict(image)\n",
    "\n",
    "        # Show image\n",
    "        import matplotlib.pyplot as plt\n",
    "        # Make subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "        # Show original image\n",
    "        axs[0].imshow(image[0])\n",
    "        axs[0].set_title('Original image')\n",
    "\n",
    "        # Show predicted image\n",
    "        axs[1].imshow(pred[0])\n",
    "        axs[1].set_title('Predicted image')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.plot(run_hist.history['accuracy'], label='train')\n",
    "plt.plot(run_hist.history['val_accuracy'], label='test')\n",
    "plt.plot(run_hist.history['loss'], label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-8inf919-aDMhdl4N-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
