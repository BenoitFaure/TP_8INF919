{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MR Tree - local configuration\n",
    "\n",
    "Srcipt for testing MR-Tree performance scalability on single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[5]\").appName(\"MySparkApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Data is sourced from https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "Steps:\n",
    "- Load data into spark dataframe\n",
    "- Preprocess data (fill na)\n",
    "- Create embedings for categorical values\n",
    "- Assemble into a single feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into Spark DataFrame\n",
    "df = spark.read.csv('data_a/adult.data', header=False, inferSchema=True)\n",
    "\n",
    "# Fill NA\n",
    "df = df.fillna(0)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and label columns\n",
    "feature_cols = df.columns[:-1]\n",
    "label_col = df.columns[-1]\n",
    "\n",
    "print(\"Feature columns: \", feature_cols)\n",
    "print(\"Label column: \", label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexers for categorical columns\n",
    "str_cols = [col for col in feature_cols if df.select(col).dtypes[0][1] == 'string']\n",
    "feature_indexers = [StringIndexer(inputCol=col, outputCol=col+'_index') for col in str_cols]\n",
    "\n",
    "# Get new feature column names\n",
    "feature_cols_indexed = [indexer.getOutputCol() for indexer in feature_indexers] + [col for col in feature_cols if col not in str_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexer for label\n",
    "labelIndexer = StringIndexer(inputCol=label_col, outputCol=\"indexedLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataframe\n",
    "df_indexed = Pipeline(stages=feature_indexers+[labelIndexer]).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.select(\"indexedLabel\", *feature_cols_indexed).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all columns to integer type\n",
    "for column_name in feature_cols_indexed + [\"indexedLabel\"]:\n",
    "    df_indexed = df_indexed.withColumn(column_name, col(column_name).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.select(\"indexedLabel\", *feature_cols_indexed).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols_indexed, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled = assembler.transform(df_indexed).select(\"features\", \"indexedLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR Tree training\n",
    "\n",
    "Check time to train MR Tree on different dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "# Init dataframe\n",
    "df_train = df_assembled.alias('df_train')\n",
    "\n",
    "# Init variables\n",
    "performances = []\n",
    "max_m = 200\n",
    "step = 10\n",
    "\n",
    "for i in range(0, max_m, step):\n",
    "    # Create the DecisionTree model\n",
    "    tree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='features')\n",
    "\n",
    "    # Fit the model to the data and calculate performance\n",
    "    dt = time.time()\n",
    "    model = tree.fit(df_train)\n",
    "    dt = time.time() - dt\n",
    "\n",
    "    # Add performance to list\n",
    "    performances.append((i + 1, dt))\n",
    "    print(performances[-1])\n",
    "\n",
    "    # Add data to train for next loop\n",
    "    for _ in range(step):\n",
    "        df_train = df_train.union(df_assembled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "with open('out/ml_tree_local.csv', 'w') as f:\n",
    "    # Write header\n",
    "    f.write('m,dt\\n')\n",
    "\n",
    "    # Write data\n",
    "    for m, dt in performances:\n",
    "        f.write(f'{m},{dt}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segment_size = 32561*14*32\n",
    "\n",
    "data_out = np.array([[m * segment_size , m, dt, dt/performances[0][1]] for m, dt in performances])\n",
    "\n",
    "# Plot the performance\n",
    "plt.plot(*zip(*performances))\n",
    "plt.xlabel('Multiplier')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the performance\n",
    "plt.plot(data_out[:, 0], data_out[:, 1], '-o')\n",
    "plt.plot(data_out[:, 0], data_out[:, 3], '-o')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Scale factor')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp-8inf919-aDMhdl4N-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
